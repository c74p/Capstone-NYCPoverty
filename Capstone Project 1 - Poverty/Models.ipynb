{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "As a refresher:\n",
    "* Data from https://data.cityofnewyork.us/browse?q=poverty\n",
    "* 12 annual data files, from 2005 to 2016 inclusive (e.g. NYCgov_Poverty_MeasureData__2016.csv)\n",
    "* CSV files with ~80 columns and ~60,000 rows each\n",
    "* Each file had essentially the same format and contained (mostly) the same information\n",
    "* Data types included:\n",
    "    * Classification types encoded as integers (e.g. 1 if in poverty, 2 if not in poverty)\n",
    "    * Floats for financial data (e.g. wages for the calendar year)\n",
    "\n",
    "I'll import a cleaned version of the files (see https://github.com/c74p/Springboard/blob/master/Capstone%20Project%201%20-%20Poverty/DataWranglingSummary.ipynb) for details.\n",
    "\n",
    "### Modeling approach\n",
    "\n",
    "The poverty rate overall in New York City is roughly 20%, and there are lots of imbalanced groups (education, income, \n",
    "disability status, etc.).  I'll use imbalanced test-train splits to improve my model.\n",
    "\n",
    "Overview of modeling approach:\n",
    "1. Use all years, households only, classify yes/no for poverty. Test and compare Logistic Regression, Support Vector \n",
    "Machines (SVM), and Random Forest algorithms.\n",
    "2. Run classifiers for individual years (the thresholds differ from year to year, so a predictor for a specific year would presumably be better for a specific year).\n",
    "3. Test running regressors on houshold income and poverty threshold, in order to predict poverty classification. Test and\n",
    "compare Linear Regression (Ordinary Least Squares), Stochastic Gradient Descent, and ElasticNet.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "4. Test steps 2 and 3 above at the person level, rather than at the household level.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "\n",
    "### Housekeeping part 1: imports and file prep\n",
    "\n",
    "After importing we'll make some quick modifications to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and setup\n",
    "# See below for model-specific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model-specific imports\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Temporarily turn off warnings if they get to be too much\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_years.csv', index_col=0)\n",
    "\n",
    "# Group the columns into 1) raw input variables, 2) id variables of various things, 3) American Community Survey (census)\n",
    "# variables, 4) NYC government-calculated variables, and 5) output variables.\n",
    "#\n",
    "# The ACS and NYC variables are generally calculated from the raw input variables - my initial expectation is that\n",
    "# the raw input variables can be thought of as independent variables, and that the ACS and NYC variables are not\n",
    "# independent even though they are not output variables.\n",
    "\n",
    "raw_inp_vars = ['AGEP', 'Boro', 'CIT', 'DIS', 'ENG', 'ESR', 'Ethnicity', 'HHT', 'HIUnit_Head', 'HousingStatus', 'JWTR', 'LANX', 'MAR', 'MSP','NP', 'Off_Threshold', 'PreTaxIncome_PU', 'REL', 'SCH', 'SCHG', 'SCHL', 'SEX', 'TEN', 'WKHP', 'WKW', 'Year']\n",
    "id_vars = ['HIUnit_ID', 'Povunit_ID', 'PWGTP', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'WGTP']\n",
    "acs_vars = ['AgeCateg', 'INTP_adj', 'OI_adj', 'MRGP_adj', 'PA_adj', 'RETP_adj', 'RNTP_adj', 'SEMP_adj', 'SSIP_adj', 'SSP_adj',  'WAGP_adj']\n",
    "nyc_vars = ['CitizenStatus',  'EducAttain', 'FTPTWork', 'FamType_PU', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_EITC', 'NYCgov_FICAtax', 'NYCgov_HEAP', 'NYCgov_Housing', 'NYCgov_Income', 'NYCgov_IncomeTax', 'NYCgov_MOOP', 'NYCgov_MedPremiums', 'NYCgov_MedSpending', 'NYCgov_Nutrition', 'NYCgov_REL', 'NYCgov_SFN', 'NYCgov_SFR', 'NYCgov_SNAP', 'NYCgov_SchoolBreakfast', 'NYCgov_SchoolLunch', 'NYCgov_Threshold', 'NYCgov_WIC', 'Povunit_Rel', 'SNAPUnit_Rel',  'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'TaxUnit_Rel', 'TotalWorkHrs_PU']\n",
    "output_vars = ['NYCgov_PovGap', 'NYCgov_Pov_Stat', 'NYCgov_PovGapIndex', 'Off_Pov_Stat']\n",
    "all_columns = raw_inp_vars + id_vars + acs_vars + nyc_vars + output_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create codes for the raw input variables that are number-coded, so we can create charts that make sense\n",
    "raw_codes = {'Boro': {1: 'Bronx', 2: 'Brooklyn', 3: 'Manhattan', 4: 'Queens', 5: 'Staten Island'},\n",
    "         'CIT': {1: 'Birth', 2: 'Territories', 3: 'US Parents', 4: 'Naturalized', 5: 'No'},\n",
    "         'DIS': {0: 'NA', 1: 'Yes', 2: 'No'},\n",
    "         'ENG': {0: '<5', 1: 'Very Well', 2: 'Well', 3: 'Not well', 4: 'Not at all', 5: 'Only Eng'},\n",
    "         'ESR': {0: '<16', 1: 'EMP', 2:'EMP/NAW', 3: 'UNEMP', 4: 'AF', 5: 'AF/NAW', 6:'NILF'},\n",
    "         'Ethnicity': {1: 'White', 2: 'Black', 3: 'Asian', 4: 'Hispanic', 5: 'Other'},\n",
    "         'HHT': {0: 'NA', 1: 'MAR', 2: 'MNW', 3: 'WNM', 4: 'Malone', 5: 'MNAlone', 6: 'Walone', 7: 'WNalone'},\n",
    "         'HIUnit_Head': {0: 'Not Head', 1: 'Head', 2: 'Not Head'},\n",
    "         'HousingStatus': {0: 'NA', 1: 'Public', 2: 'Mitchell', 3: 'Subsidy', 4: 'Regulated', 5: 'OtherReg', 6: 'MarketRate', 7: 'NoCash', 8: 'OwnF&C', 9: 'Own-Mortgage'},\n",
    "         'JWTR': {0: 'NA', 1: 'Car', 2: 'Bus', 3:'Streetcar', 4:'Subway', 5:'RR', 6:'Ferry', 7:'Taxi', 8:'Motorcycle', 9:'Bike', 10:'Walk', 11:'Home', 12: 'Other'},\n",
    "         'LANX': {0: 'NA', 1: 'Yes', 2: 'Only Eng'},\n",
    "         'MAR': {1: 'Married', 2:'Widowed', 3:'Divorced', 4:'Separated', 5:'Never Married'},\n",
    "         'MSP': {0: 'NA', 1: 'Yes', 2:'Spouse absent', 3:'Widowed', 4:'Divorced', 5:'Separated', 6:'Never Married'},\n",
    "         'REL': {0: 'Self', 1:'Spouse', 2:'Child', 3:'Adopted', 4:'Stepchild', 5:'Sibling', 6:'Parent', 7:'Grandchild', 8:'Parent-in-law', 9:'Child-in-law', 10:'Other', 11:'Boarder', 12:'Roommate', 13:'Partner', 14:'Foster', 15:'OtherNR', 16:'Inst', 17:'NonInst'},\n",
    "         'SCH': {0: 'NA', 1: 'NoPast3Mos', 2:'Public', 3:'Private/Home'},\n",
    "         'SCHG': {0: 'NA', 1:'Preschool', 2:'Kindergarten', 3:'1', 4:'2', 5:'3', 6:'4', 7:'5', 8:'6', 9:'7', 10:'8', 11:'9', 12:'10', 13:'11', 14:'12', 15:'College', 16:'Grad school'},\n",
    "         'SCHL': {0: 'NA', 1:'None', 2:'Preschool', 3:'Kindergarten', 4:'1', 5:'2', 6:'3', 7:'4', 8:'5', 9:'6', 10:'7', 11:'8', 12:'9', 13:'10', 14:'11', 15:'12-NoDip', 16:'Diploma', 17:'GED', 18:'<1yrCollege', 19:'CollNoDegree', 20:'Associates', 21:'Bachelors', 22:'Masters', 23:'Professional', 24:'Doctorate'},\n",
    "         'SEX': {1:'Male', 2:'Female'},\n",
    "         'TEN': {0: 'NA', 1:'Mortage', 2:'Free&Clear', 3:'Rent', 4:'OccButNoRent'},\n",
    "         'WKW': {0:'NA', 1:'50-52', 2:'48-49', 3:'40-47', 4:'27-39', 5:'14-26', 6:'<13'},\n",
    "        }\n",
    "\n",
    "# Create codes for the nyc variables that are number-coded, so we can create charts that make sense\n",
    "nyc_codes = {\n",
    "    'CitizenStatus': {1: 'Birth', 2: 'Naturalized', 3: 'No'},\n",
    "    'EducAttain': {0: 'NA', 1:'<HS', 2:'HS', 3:'SomeCollege', 4:'Bachelors+'},\n",
    "    'FTPTWork': {1:'FTYR', 2:'<FTYR', 3:'None'},\n",
    "    'FamType_PU': {1:'Family', 2:'Couple', 3:'M+kid', 4:'W+kid', 5:'Mnokid', 6:'Wnokid', 7:'Unrelated', 8:'UnrelAlone'},\n",
    "    'NYCgov_REL': {0:'Self', 1:'Spouse', 2:'Child', 3:'Sibling', 4:'Parent', 5:'Grandkid', 6:'Inlaw', 7:'OtherRel', 8:'Boarder', 9:'Roommate', 10:'Partner', 11:'FosterKid', 12:'OtherNonRel'},\n",
    "    'NYCgov_SFR': {0: 'NA', 1:'NoKids', 2:'Kids', 3:'OneParent', 4:'Kid', 5:'Kid-Monly', 6:'Kid-Wonly'},\n",
    "    'Povunit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'SNAPUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'TaxUnit_FILER': {1:'Filer', 0:'Non-Filer'},\n",
    "    'TaxUnit_FILESTAT': {0: 'NA', 1:'Joint', 2:'HH', 3:'MFS', 4:'Single'},\n",
    "    'TaxUnit_FILETYPE': {0: 'NA', 1: 'Normal', 2:'Dependent', 3:'BelowThresh'},\n",
    "    'TaxUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other', 5:'EIC', 6:'Relative'},\n",
    "    'TotalWorkHrs_PU': {1:'3500+', 2:'2340-3500', 3:'1750-2340', 4:'<1750', 5:'None'}\n",
    "    }\n",
    "\n",
    "# Create a dataframe 'cats' that uses categorical coding, rather than numerical coding, based on the dictionaries above.\n",
    "cats = df.replace(nyc_codes)\n",
    "cats = cats.replace(raw_codes)\n",
    "cats = cats.replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}, \n",
    "                     'Off_Pov_Stat': {1:'Pov', 2:'Not Pov'}, \n",
    "                     'AgeCateg': {1: 'U18', 2:'18-64', 3:'65+'}})\n",
    "\n",
    "# Update one column so that NA's are all in one category\n",
    "cats.loc[cats['HIUnit_Head'].isna(), 'HIUnit_Head'] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key dataframes of interest\n",
    "\n",
    "# cats is already listed above - dataframe with all of the category variables un-encoded\n",
    "\n",
    "# All 2016 data\n",
    "all_2016 = df[df.Year == 2016]\n",
    "cats_2016 = cats[cats.Year == 2016]\n",
    "\n",
    "# 2016 data for poverty units only\n",
    "# For example, the data dictionary, sheet \"Column Info\", cell D81, says in relation to calculating the poverty gap:\n",
    "# \"retain only the reference person of each family in poverty (Povunit_Rel==1 & NYCgov_Poverty == 1)\"\n",
    "pu_2016 = df[(df.Year == 2016) & (df.Povunit_Rel == 1)]\n",
    "pu_cats_2016 = cats[(cats.Year == 2016) & (cats.Povunit_Rel == 'Head')]\n",
    "\n",
    "# Our data set contains two sets of weights: household weights and person weights.  \n",
    "# We need to separate out each column by whether it should be weighted as a household variable or a person variable.\n",
    "# Lists to create weighted columns, separated based on whether they are personal or household statistics.\n",
    "personal_vars = ['AGEP', 'Boro', 'CIT', 'SCH', 'SCHG', 'SCHL', 'SEX', 'ESR', 'ENG', 'LANX', 'MSP', 'MAR', 'NYCgov_EITC', 'WKW', 'WKHP', 'DIS', 'JWTR', 'WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj', 'TaxUnit_Rel', 'NYCgov_REL', 'NYCgov_SFR', 'SNAPUnit_Rel', 'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'Ethnicity', 'EducAttain', 'CitizenStatus', 'AgeCateg', 'FTPTWork', 'PWGTP'] \n",
    "pu_vars = ['MRGP_adj', 'RNTP_adj', 'NP', 'TEN', 'HHT', 'FamType_PU', 'HousingStatus', 'TotalWorkHrs_PU', 'PreTaxIncome_PU', 'NYCgov_Income', 'NYCgov_Threshold', 'NYCgov_Pov_Stat',  'NYCgov_Housing', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_MOOP', 'NYCgov_MedSpending', 'NYCgov_MedPremiums', 'NYCgov_HEAP', 'NYCgov_WIC', 'NYCgov_SNAP', 'NYCgov_SchoolLunch', 'NYCgov_SchoolBreakfast', 'NYCgov_Nutrition', 'NYCgov_FICAtax', 'NYCgov_IncomeTax', 'Off_Threshold', 'Off_Pov_Stat', 'NYCgov_PovGap', 'NYCgov_PovGapIndex', 'WGTP']\n",
    "other_vars = ['HIUnit_Head', 'HIUnit_ID', 'NYCgov_SFN', 'Povunit_ID', 'Povunit_Rel', 'REL', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'Year']\n",
    "\n",
    "# Need to list out categorical variables for transformation to 'category' type and one-hot encoding\n",
    "# Do this for different scenarios (feature selection) as part of the test of which is most predictive\n",
    "pu_all_categoricals = ['AGEP', 'Boro', 'CIT', 'DIS', 'ENG', 'ESR', 'Ethnicity', 'HHT', 'HIUnit_Head', 'HousingStatus', 'JWTR', 'LANX', 'MAR', 'MSP', 'NP', 'REL', 'SCH', 'SCHG', 'SCHL', 'SEX', 'TEN', 'WKHP', 'WKW', 'CitizenStatus', 'EducAttain', 'FTPTWork', 'FamType_PU', 'NYCgov_HEAP', 'NYCgov_REL', 'NYCgov_SFN', 'NYCgov_SFR', 'Povunit_Rel', 'SNAPUnit_Rel', 'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'TaxUnit_Rel', 'TotalWorkHrs_PU', 'AgeCateg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's hit it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 0 0 0]\n",
      "Actual:\n",
      "710625    0\n",
      "710627    1\n",
      "710629    0\n",
      "710630    0\n",
      "710631    0\n",
      "Name: NYCgov_Pov_Stat, dtype: int64\n",
      "\n",
      "Balanced accuracy: 1.0\n",
      "Geometric mean: 1.0\n",
      "Confusion matrix:\n",
      "[[4 0]\n",
      " [0 1]]\n",
      "\n",
      "Classification report:\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00         4\n",
      "          1       1.00      1.00      1.00      1.00      1.00      1.00         1\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1.00      1.00      1.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start with 2016 data only, household('poverty unit')-level\n",
    "y = pu_cats_2016['NYCgov_Pov_Stat'].replace({'Pov': 1, 'Not Pov': 0})\n",
    "X = pu_cats_2016.drop(['NYCgov_Pov_Stat', 'Year'], axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "categorizer = Categorizer(columns=pu_all_categoricals)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "sampler = TomekLinks(random_state=42)\n",
    "scaler = QuantileTransformer()\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "X_small = X.iloc[5:10,:]\n",
    "y_small = y.iloc[5:10]\n",
    "#test = Pipeline(steps=[('cat', categorizer), \n",
    "test = imbPipeline(steps=[('cat', categorizer), \n",
    "                       ('dummies', dummy_encoder),\n",
    "                       ('sampler', sampler),\n",
    "                       ('scaler', scaler),\n",
    "                       ('clf', clf)])\n",
    "test.fit(X_small, y_small)\n",
    "predictions = test.predict(X_small)\n",
    "print('Predictions: ' + str(predictions))\n",
    "print('Actual:\\n' + str(y_small))\n",
    "\n",
    "print('\\nBalanced accuracy: ' + str(balanced_accuracy_score(y_small, predictions)))\n",
    "print('Geometric mean: ' + str(geometric_mean_score(y_small, predictions)))\n",
    "print('Confusion matrix:\\n' + str(confusion_matrix(y_small, predictions)))\n",
    "print('\\nClassification report:\\n' + str(classification_report_imbalanced(y_small, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
