{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "As a refresher:\n",
    "* Data from https://data.cityofnewyork.us/browse?q=poverty\n",
    "* 12 annual data files, from 2005 to 2016 inclusive (e.g. NYCgov_Poverty_MeasureData__2016.csv)\n",
    "* CSV files with ~80 columns and ~60,000 rows each\n",
    "* Each file had essentially the same format and contained (mostly) the same information\n",
    "* Data types included:\n",
    "    * Classification types encoded as integers (e.g. 1 if in poverty, 2 if not in poverty)\n",
    "    * Floats for financial data (e.g. wages for the calendar year)\n",
    "\n",
    "I'll import a cleaned version of the files (see https://github.com/c74p/Springboard/blob/master/Capstone%20Project%201%20-%20Poverty/DataWranglingSummary.ipynb) for details.\n",
    "\n",
    "### Modeling approach\n",
    "\n",
    "The poverty rate overall in New York City is roughly 20%, and there are lots of imbalanced groups (education, income, \n",
    "disability status, etc.).  I'll use imbalanced test-train splits to improve my model.\n",
    "\n",
    "Overview of modeling approach:\n",
    "1. Use all years, households only, classify yes/no for poverty. Test and compare Logistic Regression, Support Vector \n",
    "Machines (SVM), and Random Forest algorithms.\n",
    "2. Run classifiers for individual years (the thresholds differ from year to year, so a predictor for a specific year would presumably be better for a specific year).\n",
    "3. Test running regressors on houshold income and poverty threshold, in order to predict poverty classification. Test and\n",
    "compare Linear Regression (Ordinary Least Squares), Stochastic Gradient Descent, and ElasticNet.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "4. Test steps 2 and 3 above at the person level, rather than at the household level.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "\n",
    "### Housekeeping part 1: imports and file prep\n",
    "\n",
    "After importing we'll make some quick modifications to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and setup\n",
    "# See below for model-specific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Model-specific imports\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Logit, LogitResults\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Temporarily turn off warnings if they get to be too much\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_years.csv', index_col=0)\n",
    "\n",
    "# Group the columns into 1) raw input variables, 2) id variables of various things, 3) American Community Survey (census)\n",
    "# variables, 4) NYC government-calculated variables, and 5) output variables.\n",
    "#\n",
    "# The ACS and NYC variables are generally calculated from the raw input variables - my initial expectation is that\n",
    "# the raw input variables can be thought of as independent variables, and that the ACS and NYC variables are not\n",
    "# independent even though they are not output variables.\n",
    "\n",
    "raw_inp_vars = ['AGEP', 'Boro', 'CIT', 'DIS', 'ENG', 'ESR', 'Ethnicity', 'HHT', 'HIUnit_Head', 'HousingStatus', 'JWTR', 'LANX', 'MAR', 'MSP','NP', 'Off_Threshold', 'PreTaxIncome_PU', 'REL', 'SCH', 'SCHG', 'SCHL', 'SEX', 'TEN', 'WKHP', 'WKW', 'Year']\n",
    "id_vars = ['HIUnit_ID', 'Povunit_ID', 'PWGTP', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'WGTP']\n",
    "acs_vars = ['AgeCateg', 'INTP_adj', 'OI_adj', 'MRGP_adj', 'PA_adj', 'RETP_adj', 'RNTP_adj', 'SEMP_adj', 'SSIP_adj', 'SSP_adj',  'WAGP_adj']\n",
    "nyc_vars = ['CitizenStatus',  'EducAttain', 'FTPTWork', 'FamType_PU', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_EITC', 'NYCgov_FICAtax', 'NYCgov_HEAP', 'NYCgov_Housing', 'NYCgov_Income', 'NYCgov_IncomeTax', 'NYCgov_MOOP', 'NYCgov_MedPremiums', 'NYCgov_MedSpending', 'NYCgov_Nutrition', 'NYCgov_REL', 'NYCgov_SFN', 'NYCgov_SFR', 'NYCgov_SNAP', 'NYCgov_SchoolBreakfast', 'NYCgov_SchoolLunch', 'NYCgov_Threshold', 'NYCgov_WIC', 'Povunit_Rel', 'SNAPUnit_Rel',  'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'TaxUnit_Rel', 'TotalWorkHrs_PU']\n",
    "output_vars = ['NYCgov_PovGap', 'NYCgov_Pov_Stat', 'NYCgov_PovGapIndex', 'Off_Pov_Stat']\n",
    "all_columns = raw_inp_vars + id_vars + acs_vars + nyc_vars + output_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create codes for the raw input variables that are number-coded, so we can create charts that make sense\n",
    "raw_codes = {'Boro': {1: 'Bronx', 2: 'Brooklyn', 3: 'Manhattan', 4: 'Queens', 5: 'Staten Island'},\n",
    "         'CIT': {1: 'Birth', 2: 'Territories', 3: 'US Parents', 4: 'Naturalized', 5: 'No'},\n",
    "         'DIS': {0: 'NA', 1: 'Yes', 2: 'No'},\n",
    "         'ENG': {0: '<5', 1: 'Very Well', 2: 'Well', 3: 'Not well', 4: 'Not at all', 5: 'Only Eng'},\n",
    "         'ESR': {0: '<16', 1: 'EMP', 2:'EMP/NAW', 3: 'UNEMP', 4: 'AF', 5: 'AF/NAW', 6:'NILF'},\n",
    "         'Ethnicity': {1: 'White', 2: 'Black', 3: 'Asian', 4: 'Hispanic', 5: 'Other'},\n",
    "         'HHT': {0: 'NA', 1: 'MAR', 2: 'MNW', 3: 'WNM', 4: 'Malone', 5: 'MNAlone', 6: 'Walone', 7: 'WNalone'},\n",
    "         'HIUnit_Head': {0: 'Not Head', 1: 'Head', 2: 'Not Head'},\n",
    "         'HousingStatus': {0: 'NA', 1: 'Public', 2: 'Mitchell', 3: 'Subsidy', 4: 'Regulated', 5: 'OtherReg', 6: 'MarketRate', 7: 'NoCash', 8: 'OwnF&C', 9: 'Own-Mortgage'},\n",
    "         'JWTR': {0: 'NA', 1: 'Car', 2: 'Bus', 3:'Streetcar', 4:'Subway', 5:'RR', 6:'Ferry', 7:'Taxi', 8:'Motorcycle', 9:'Bike', 10:'Walk', 11:'Home', 12: 'Other'},\n",
    "         'LANX': {0: 'NA', 1: 'Yes', 2: 'Only Eng'},\n",
    "         'MAR': {1: 'Married', 2:'Widowed', 3:'Divorced', 4:'Separated', 5:'Never Married'},\n",
    "         'MSP': {0: 'NA', 1: 'Yes', 2:'Spouse absent', 3:'Widowed', 4:'Divorced', 5:'Separated', 6:'Never Married'},\n",
    "         'REL': {0: 'Self', 1:'Spouse', 2:'Child', 3:'Adopted', 4:'Stepchild', 5:'Sibling', 6:'Parent', 7:'Grandchild', 8:'Parent-in-law', 9:'Child-in-law', 10:'Other', 11:'Boarder', 12:'Roommate', 13:'Partner', 14:'Foster', 15:'OtherNR', 16:'Inst', 17:'NonInst'},\n",
    "         'SCH': {0: 'NA', 1: 'NoPast3Mos', 2:'Public', 3:'Private/Home'},\n",
    "         'SCHG': {0: 'NA', 1:'Preschool', 2:'Kindergarten', 3:'1', 4:'2', 5:'3', 6:'4', 7:'5', 8:'6', 9:'7', 10:'8', 11:'9', 12:'10', 13:'11', 14:'12', 15:'College', 16:'Grad school'},\n",
    "         'SCHL': {0: 'NA', 1:'None', 2:'Preschool', 3:'Kindergarten', 4:'1', 5:'2', 6:'3', 7:'4', 8:'5', 9:'6', 10:'7', 11:'8', 12:'9', 13:'10', 14:'11', 15:'12-NoDip', 16:'Diploma', 17:'GED', 18:'<1yrCollege', 19:'CollNoDegree', 20:'Associates', 21:'Bachelors', 22:'Masters', 23:'Professional', 24:'Doctorate'},\n",
    "         'SEX': {1:'Male', 2:'Female'},\n",
    "         'TEN': {0: 'NA', 1:'Mortage', 2:'Free&Clear', 3:'Rent', 4:'OccButNoRent'},\n",
    "         'WKW': {0:'NA', 1:'50-52', 2:'48-49', 3:'40-47', 4:'27-39', 5:'14-26', 6:'<13'},\n",
    "        }\n",
    "\n",
    "# Create codes for the nyc variables that are number-coded, so we can create charts that make sense\n",
    "nyc_codes = {\n",
    "    'CitizenStatus': {1: 'Birth', 2: 'Naturalized', 3: 'No'},\n",
    "    'EducAttain': {0: 'NA', 1:'<HS', 2:'HS', 3:'SomeCollege', 4:'Bachelors+'},\n",
    "    'FTPTWork': {1:'FTYR', 2:'<FTYR', 3:'None'},\n",
    "    'FamType_PU': {1:'Family', 2:'Couple', 3:'M+kid', 4:'W+kid', 5:'Mnokid', 6:'Wnokid', 7:'Unrelated', 8:'UnrelAlone'},\n",
    "    'NYCgov_REL': {0:'Self', 1:'Spouse', 2:'Child', 3:'Sibling', 4:'Parent', 5:'Grandkid', 6:'Inlaw', 7:'OtherRel', 8:'Boarder', 9:'Roommate', 10:'Partner', 11:'FosterKid', 12:'OtherNonRel'},\n",
    "    'NYCgov_SFR': {0: 'NA', 1:'NoKids', 2:'Kids', 3:'OneParent', 4:'Kid', 5:'Kid-Monly', 6:'Kid-Wonly'},\n",
    "    'Povunit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'SNAPUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'TaxUnit_FILER': {1:'Filer', 0:'Non-Filer'},\n",
    "    'TaxUnit_FILESTAT': {0: 'NA', 1:'Joint', 2:'HH', 3:'MFS', 4:'Single'},\n",
    "    'TaxUnit_FILETYPE': {0: 'NA', 1: 'Normal', 2:'Dependent', 3:'BelowThresh'},\n",
    "    'TaxUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other', 5:'EIC', 6:'Relative'},\n",
    "    'TotalWorkHrs_PU': {1:'3500+', 2:'2340-3500', 3:'1750-2340', 4:'<1750', 5:'None'}\n",
    "    }\n",
    "\n",
    "# Create a dataframe 'cats' that uses categorical coding, rather than numerical coding, based on the dictionaries above.\n",
    "#cats = df.replace(nyc_codes)\n",
    "#cats = cats.replace(raw_codes)\n",
    "#cats = cats.replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}, \n",
    "                     #'Off_Pov_Stat': {1:'Pov', 2:'Not Pov'}, \n",
    "                     #'AgeCateg': {1: 'U18', 2:'18-64', 3:'65+'}})\n",
    "\n",
    "# Update one column so that NA's are all in one category\n",
    "#cats.loc[cats['HIUnit_Head'].isna(), 'HIUnit_Head'] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key dataframes of interest\n",
    "\n",
    "# cats is already listed above - dataframe with all of the category variables un-encoded\n",
    "\n",
    "# All 2016 data\n",
    "all_2016 = df[df.Year == 2016]\n",
    "#cats_2016 = cats[cats.Year == 2016]\n",
    "\n",
    "# 2016 data for poverty units only\n",
    "# For example, the data dictionary, sheet \"Column Info\", cell D81, says in relation to calculating the poverty gap:\n",
    "# \"retain only the reference person of each family in poverty (Povunit_Rel==1 & NYCgov_Poverty == 1)\"\n",
    "#pu_2016 = df[(df.Year == 2016) & (df.Povunit_Rel == 1)]\n",
    "#pu_cats_2016 = cats[(cats.Year == 2016) & (cats.Povunit_Rel == 'Head')]\n",
    "#pu_cats_all_years = cats[(cats.Povunit_Rel == 'Head')]\n",
    "\n",
    "# Our data set contains two sets of weights: household weights and person weights.  \n",
    "# We need to separate out each column by whether it should be weighted as a household variable or a person variable.\n",
    "# Lists to create weighted columns, separated based on whether they are personal or household statistics.\n",
    "personal_vars = ['AGEP', 'Boro', 'CIT', 'SCH', 'SCHG', 'SCHL', 'SEX', 'ESR', 'ENG', 'LANX', 'MSP', 'MAR', 'NYCgov_EITC', 'WKW', 'WKHP', 'DIS', 'JWTR', 'WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj', 'TaxUnit_Rel', 'NYCgov_REL', 'NYCgov_SFR', 'SNAPUnit_Rel', 'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'Ethnicity', 'EducAttain', 'CitizenStatus', 'AgeCateg', 'FTPTWork', 'PWGTP'] \n",
    "pu_vars = ['MRGP_adj', 'RNTP_adj', 'NP', 'TEN', 'HHT', 'FamType_PU', 'HousingStatus', 'TotalWorkHrs_PU', 'PreTaxIncome_PU', 'NYCgov_Income', 'NYCgov_Threshold', 'NYCgov_Pov_Stat',  'NYCgov_Housing', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_MOOP', 'NYCgov_MedSpending', 'NYCgov_MedPremiums', 'NYCgov_HEAP', 'NYCgov_WIC', 'NYCgov_SNAP', 'NYCgov_SchoolLunch', 'NYCgov_SchoolBreakfast', 'NYCgov_Nutrition', 'NYCgov_FICAtax', 'NYCgov_IncomeTax', 'Off_Threshold', 'Off_Pov_Stat', 'NYCgov_PovGap', 'NYCgov_PovGapIndex', 'WGTP']\n",
    "other_vars = ['HIUnit_Head', 'HIUnit_ID', 'NYCgov_SFN', 'Povunit_ID', 'Povunit_Rel', 'REL', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Adding New Poverty-Unit Variables Based on Personal Variables\n",
    "Here's where we're actually adding new variables based on old variables.\n",
    "\n",
    "From the personal-level features, we'll create new household-level features (e.g. number of kids, mean salary among \n",
    "adults, count of people making more than $30k, count of adults working between 14-26 hours/week, minimum salary among\n",
    "adults working more than 26 hours/week, etc.)\n",
    "\n",
    "This is a wall of text, not much exciting narrative here.\n",
    "\n",
    "(Note on programming style/choices: the first function 'add_pu_columns' below is a real function, although it has a\n",
    "kludge in case one of the summary calculations fails. (That happens rarely, but it's still a code smell.)\n",
    "\n",
    "The second function, however, is not really a 'real' function; it's heavily hand-coded and relies on custom choices of\n",
    "various groupings.  It was put into a function solely to comply with the DRY principle - in particular, this function\n",
    "will typically be run twice (once for the whole grouping, and possibly once for a model run without the 'financial' \n",
    "features).  By putting it into a function for DRY purposes, we at least avoid accidentally running different code when we \n",
    "mean to run the same code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pu_columns(df, groups, group_names, categories, category_names, column):\n",
    "    \"\"\"\n",
    "    Adds columns to dataframe 'df' containing calculations by poverty-unit, restricted by categories, considering groups.\n",
    "    Calculations include any(), all(), min(), max(), count(), sum(), mean(), and % in given category.\n",
    "    Input: a dataframe with multi-index consisting of 'SERIALNO', 'Povunit_ID', and 'SPORDER'; a set of masks and list of\n",
    "    names for the groups; a set of masks and a list of names for the categories; and the column of interest.\n",
    "    Output: no return value.  Inserts a series of columns into the dataframe including min, max, count, sum, any, all,\n",
    "    % of total, and mean - within households, focusing on the groups and categories of interest. \n",
    "    \"\"\"\n",
    "    \n",
    "    for group, group_name in zip(groups, group_names):\n",
    "        for category, category_name in zip(categories, category_names):\n",
    "            stacked = df[column][group & category].stack().groupby(['SERIALNO', 'Povunit_ID'])\n",
    "            anys = stacked.any()\n",
    "            # would love to use .all() here, but it would always be True because we filtered out everyone else\n",
    "            mins = stacked.min()\n",
    "            maxes = stacked.max()\n",
    "            counts = stacked.count()\n",
    "            sums = stacked.sum()\n",
    "            means = sums/counts\n",
    "            # The divisor below only restricts by 'groups' - so the final calculation is within a household, within\n",
    "            # the group of interest (e.g. adults), what % is in the category of interest (e.g. works 40 hrs/week)\n",
    "            divisor_for_percents = df[column][group].stack().groupby(['SERIALNO', 'Povunit_ID']).count()\n",
    "            try:\n",
    "                percents = counts.div(divisor_for_percents, axis=0)\n",
    "                alls = percents == 1\n",
    "            except: # if the calculation failed, leave percents and alls as a column of zeros\n",
    "                df_len = len(df.groupby(['SERIALNO', 'Povunit_ID']).sum())\n",
    "                percents = np.zeros(df_len)\n",
    "                alls = np.zeros(df_len)\n",
    "            \n",
    "            # loop through, put in the dataframe, and fill in NAs of appropriate type\n",
    "            series_and_names = zip([anys, alls, mins, maxes, counts, sums, means, percents], \n",
    "                                  ['any', 'all', 'min', 'max', 'count', 'sum', 'mean', '%'])\n",
    "            for series, series_name in series_and_names:\n",
    "                column_title = series_name + '_' + group_name + '_' + category_name\n",
    "                df[column_title] = series\n",
    "                if series_name in ['any', 'all']:\n",
    "                    df[column_title] = df[column_title].fillna(False)\n",
    "                else:\n",
    "                    df[column_title] = df[column_title].fillna(0)\n",
    "                    \n",
    "def engineer_features(df, include_financials=True):\n",
    "    \"\"\"Create features for the dataframe. This function is heavily custom and was solely created for DRY-ness.\n",
    "    Input: a poverty dataframe and whether or not to include financial features.\n",
    "    Output: returns a copy of the dataframe summarized by poverty-unit, with *only* the new features included. \n",
    "    Prints progess updates to the screen as it goes.\n",
    "    \"\"\"\n",
    "\n",
    "    time_0 = time.time()\n",
    "\n",
    "    # Create dataframe to house new features \n",
    "    dfc = df.copy() # read as 'X new features'\n",
    "\n",
    "    # Count the original # of features - later we'll just slice out the new features\n",
    "    # Keep in mind that 'SERIALNO' and 'Povunit_ID' will go into the index, so we need to subtract 2 columns\n",
    "    features_to_mask = len(dfc.columns) - 2\n",
    "    \n",
    "    # This is the largest # of people in a household; when we group the columns below, for each existing feature we'll\n",
    "    # create one column for each person in a household - so we'll need to know this number at the end when we want to \n",
    "    # mask our existing features\n",
    "    max_ppl = dfc.SPORDER.max()\n",
    "\n",
    "    # First, some categoricals have odd ordering; remap them\n",
    "    fix_orders = {'ENG': {0:0, 4:1, 3:2, 2:3, 1:4, 5:5}, 'WKW': {0:0, 6:1, 5:2, 4:3, 3:4, 2:5, 1:6}, \n",
    "                  'TotalWorkHrs_PU': {5:0, 4:1, 3:2, 2:3, 1:4}}\n",
    "    dfc['ENG'] = dfc['ENG'].map(fix_orders['ENG'])\n",
    "    dfc['WKW'] = dfc['WKW'].map(fix_orders['WKW'])\n",
    "    dfc['TotalWorkHrs_PU'] = dfc['TotalWorkHrs_PU'].map(fix_orders['TotalWorkHrs_PU'])\n",
    "\n",
    "    # Add column for total personal income\n",
    "    if include_financials:\n",
    "        dfc['TINP'] = dfc.WAGP_adj + dfc.INTP_adj + dfc.SEMP_adj + dfc.SSP_adj + dfc.SSIP_adj + \\\n",
    "                        dfc.PA_adj + dfc.RETP_adj + dfc.OI_adj\n",
    "\n",
    "    # Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "    dfc = dfc.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "    # Create masks for age groups to use in creating new features\n",
    "    mask_adult = (dfc.AgeCateg == 2) | (dfc.AgeCateg == 3)\n",
    "    mask_65_plus = dfc.AgeCateg == 3\n",
    "    mask_18_64 = dfc.AgeCateg == 2\n",
    "    mask_kid = dfc.AgeCateg == 1\n",
    "    mask_any_age = dfc.AgeCateg != 0\n",
    "    mask_any = mask_any_age\n",
    "\n",
    "    # add columns with age only, no categories\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "    add_pu_columns(dfc, groups, group_names, [mask_any_age], ['age'], 'AGEP')\n",
    "\n",
    "    # add columns for CIT\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_non_cit = dfc.CIT == 5\n",
    "    mask_cit = (dfc.CIT != 5) & (dfc.CIT != 0)\n",
    "    mask_naturalized = dfc.CIT == 4\n",
    "\n",
    "    categories = [mask_non_cit, mask_cit, mask_naturalized, mask_any]\n",
    "    category_names = ['non-cit', 'citizen', 'naturalized_cit', 'any_CIT']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'CIT')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with CIT')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for SCHL\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_college_degree = (dfc.SCHL >= 21)\n",
    "    mask_HS_diploma = (dfc.SCHL >= 17)\n",
    "    mask_no_diploma = (dfc.SCHL <= 16)\n",
    "\n",
    "    categories = [mask_college_degree, mask_HS_diploma, mask_no_diploma, mask_HS_diploma & ~mask_college_degree, mask_any]\n",
    "    category_names = ['college', 'HS', 'no_diploma', 'diploma_no_bachelors', 'any_SCHL']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'SCHL')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with SCHL')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for SEX\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_male = dfc.SEX == 1\n",
    "    mask_female = dfc.SEX == 2\n",
    "\n",
    "    categories = [mask_male, mask_female, mask_any]\n",
    "    category_names = ['male', 'female', 'any_SEX']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'SEX')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with SEX')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for English ability (ENG)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # Keep in mind we switched ENG above so that 0 is NA, 1 is not at all, 2 is not very well, ..., 5 is only English\n",
    "    mask_no_english = dfc.ENG == 1\n",
    "    mask_eng_nvw = dfc.ENG == 2\n",
    "    mask_sep_well = dfc.ENG == 3\n",
    "    mask_eng_vw = dfc.ENG == 4\n",
    "    mask_only_eng = dfc.ENG == 5\n",
    "\n",
    "    categories = [mask_no_english, mask_eng_nvw, mask_sep_well, mask_eng_vw, mask_only_eng, mask_any]\n",
    "    category_names = ['ENG_no', 'ENG_nvw', 'ENG_well', 'ENG_vw', 'ENG_only', 'ENG_any']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'ENG')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with ENG')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for marital status (MSP)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_married = (dfc.MSP == 1) | (dfc.MSP == 2)\n",
    "    mask_widowed = dfc.MSP == 3\n",
    "    mask_sep_div = (dfc.MSP == 4) | (dfc.MSP == 5)\n",
    "    mask_not_married = dfc.MSP == 6\n",
    "\n",
    "    categories = [mask_married, mask_widowed, mask_sep_div, mask_not_married, mask_any]\n",
    "    category_names = ['married', 'widowed', 'sep/divorced', 'not_married', 'any_MSP']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'MSP')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with MSP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for weeks worked (WKW) -- this is *weeks* worked last year, not *hours per week* (that's WKHP)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # Keep in mind we switched WKW above so that 0 is none, 1 is <14 weeks, 2 is 14-26 weeks, etc.\n",
    "    mask_0_WKW = dfc.WKW == 0\n",
    "    mask_u14_WKW = dfc.WKW == 1\n",
    "    mask_14_26_WKW = dfc.WKW == 2\n",
    "    mask_27_39_WKW = dfc.WKW == 3\n",
    "    mask_40_47_WKW = dfc.WKW == 4\n",
    "    mask_48_49_WKW = dfc.WKW == 5\n",
    "    mask_50_52_WKW = dfc.WKW == 6\n",
    "\n",
    "    categories = [mask_0_WKW, mask_u14_WKW, mask_14_26_WKW, mask_27_39_WKW, mask_40_47_WKW, mask_48_49_WKW, mask_50_52_WKW, \n",
    "                 (mask_40_47_WKW | mask_48_49_WKW | mask_50_52_WKW), ~mask_0_WKW, mask_any]\n",
    "    category_names = ['no_work', '<14WKW', '14-26WKW', '27-39WKW', '40-47WKW', '48-49WKW', '50-52WKW', '>40WKW', 'nonzero_WKW',\n",
    "                     'any_WKW']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'WKW')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with WKW')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for usual hours worked per week last 12 months (WKHP)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_0_WKHP = dfc.WKHP == 0\n",
    "    mask_u10_WKHP = dfc.WKHP < 10\n",
    "    mask_u15_WKHP = dfc.WKHP < 15\n",
    "    mask_u20_WKHP = dfc.WKHP < 20\n",
    "    mask_u30_WKHP = dfc.WKHP < 30\n",
    "    mask_u40_WKHP = dfc.WKHP < 40\n",
    "    mask_u50_WKHP = dfc.WKHP < 50\n",
    "    mask_50_plus_WKHP = dfc.WKHP >= 50\n",
    "    mask_40_plus_WKHP = dfc.WKHP >= 40\n",
    "\n",
    "    categories = [mask_0_WKHP, mask_u10_WKHP, mask_u15_WKHP, mask_u20_WKHP, mask_u30_WKHP, mask_u40_WKHP, \n",
    "                  mask_u50_WKHP, mask_50_plus_WKHP, mask_40_plus_WKHP, mask_any]\n",
    "    category_names = ['no_work_hrs', '<10_work_hrs', '<15_work_hrs', '<20_work_hrs', '<30_work_hrs', '<40_work_hrs', \n",
    "                      '<50_work_hrs', '50_plus_work_hrs', '40_plus_work_hrs', 'any_WKHP']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'WKHP')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with WKHP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for disability status (DIS)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_DIS = dfc.DIS == 1\n",
    "    mask_not_DIS = dfc.DIS == 2\n",
    "\n",
    "    categories = [mask_DIS, mask_not_DIS, mask_any]\n",
    "    category_names = ['DIS', 'not_DIS', 'any_DIS']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'DIS')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with DIS')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for number of people (NP)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_1_NP = dfc.NP == 1\n",
    "    mask_2_NP = dfc.NP == 2\n",
    "    mask_3_NP = dfc.NP == 3\n",
    "    mask_4_NP = dfc.NP == 4\n",
    "    mask_5_NP = dfc.NP == 5\n",
    "    mask_p5_NP = dfc.NP > 5\n",
    "    mask_p6_NP = dfc.NP > 6\n",
    "    mask_p8_NP = dfc.NP > 8\n",
    "    mask_p10_NP = dfc.NP > 10\n",
    "    mask_p12_NP = dfc.NP > 12\n",
    "\n",
    "    categories = [mask_1_NP, mask_2_NP, mask_3_NP, mask_4_NP, mask_5_NP, mask_p5_NP, mask_p6_NP, mask_p8_NP, mask_p10_NP, \n",
    "                  mask_p12_NP, mask_any]\n",
    "    category_names = ['NP1', 'NP2', 'NP3', 'NP4', 'NP5', 'NP>5', 'NP>6', 'NP>8', 'NP>10', 'NP>12', 'anyNP']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'NP')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with NP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for means of transportation to work (JWTR)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    categories = [mask_any]\n",
    "    category_names = ['work_trans']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'JWTR')\n",
    "    # Since only doing this to get means/avgs on JWTR, no need to add the 'AGEP' version here\n",
    "    #add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with JWTR')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for wages (WAGP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_0_WAG = dfc.WAGP_adj == 0\n",
    "    mask_u10_WAG = dfc.WAGP_adj < 10000\n",
    "    mask_u15_WAG = dfc.WAGP_adj < 15000\n",
    "    mask_u20_WAG = dfc.WAGP_adj < 20000\n",
    "    mask_u25_WAG = dfc.WAGP_adj < 25000\n",
    "    mask_u30_WAG = dfc.WAGP_adj < 30000\n",
    "    mask_u35_WAG = dfc.WAGP_adj < 35000\n",
    "    mask_u40_WAG = dfc.WAGP_adj < 40000\n",
    "    mask_u45_WAG = dfc.WAGP_adj < 45000\n",
    "    mask_u50_WAG = dfc.WAGP_adj < 50000\n",
    "    mask_u60_WAG = dfc.WAGP_adj < 60000\n",
    "    mask_u70_WAG = dfc.WAGP_adj < 70000\n",
    "    mask_u80_WAG = dfc.WAGP_adj < 80000\n",
    "\n",
    "    categories = [mask_0_WAG, mask_u10_WAG, mask_u15_WAG, mask_u20_WAG, mask_u25_WAG, mask_u30_WAG,  mask_u35_WAG, \n",
    "                  mask_u40_WAG, mask_u45_WAG, mask_u50_WAG, mask_u60_WAG, mask_u70_WAG, mask_u80_WAG, mask_any]\n",
    "    category_names = ['WAG0', 'WAG<10', 'WAG<15', 'WAG<20', 'WAG<25', 'WAG<30', 'WAG<35', \n",
    "                      'WAG<40', 'WAG<45', 'WAG<50', 'WAG<60', 'WAG<70', 'WAG<80', 'WAG_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'WAGP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with WAGP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for interest income (INTP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_0_INT = dfc.INTP_adj <= 0\n",
    "    mask_INT_1q = (dfc.INTP_adj > 0) & (dfc.INTP_adj <= 400)\n",
    "    mask_INT_2q = (dfc.INTP_adj > 400) & (dfc.INTP_adj <= 4000)\n",
    "    mask_INT_3q = (dfc.INTP_adj > 4000) & (dfc.INTP_adj <= 15000)\n",
    "    mask_INT_4q = dfc.INTP_adj > 15000\n",
    "\n",
    "    categories = [mask_0_INT, mask_INT_1q, mask_INT_2q, mask_INT_3q, mask_INT_4q, mask_any]\n",
    "    category_names = ['INT0', 'INT1q', 'INT2q', 'INT3q', 'INT4q', 'INT_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'INTP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with INTP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for self-employment income (SEMP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_0_SEMP = dfc.SEMP_adj <= 0\n",
    "    mask_SEMP_1q = (dfc.SEMP_adj > 0) & (dfc.SEMP_adj <= 5000)\n",
    "    mask_SEMP_2q = (dfc.SEMP_adj > 5000) & (dfc.SEMP_adj <= 15000)\n",
    "    mask_SEMP_3q = (dfc.SEMP_adj > 15000) & (dfc.SEMP_adj <= 35000)\n",
    "    mask_SEMP_4q = dfc.SEMP_adj > 35000\n",
    "\n",
    "    categories = [mask_0_SEMP, mask_SEMP_1q, mask_SEMP_2q, mask_SEMP_3q, mask_SEMP_4q, mask_any]\n",
    "    category_names = ['SEMP0', 'SEMP1q', 'SEMP2q', 'SEMP3q', 'SEMP4q', 'SEMP_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'SEMP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with SEMP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for social security income (SSP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    # min 10, 25% 8000, 50% 12,000, 75% 18,000, max 50,000\n",
    "    mask_0_SSP = dfc.SSP_adj <= 0\n",
    "    mask_SSP_1q = (dfc.SSP_adj > 0) & (dfc.SSP_adj <= 8000)\n",
    "    mask_SSP_2q = (dfc.SSP_adj > 8000) & (dfc.SSP_adj <= 12000)\n",
    "    mask_SSP_3q = (dfc.SSP_adj > 12000) & (dfc.SSP_adj <= 18000)\n",
    "    mask_SSP_4q = dfc.SSP_adj > 18000\n",
    "\n",
    "    categories = [mask_0_SSP, mask_SSP_1q, mask_SSP_2q, mask_SSP_3q, mask_SSP_4q, mask_any]\n",
    "    category_names = ['SSP0', 'SSP1q', 'SSP2q', 'SSP3q', 'SSP4q', 'SSP_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'SSP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with SSP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for supplemental security income (SSIP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_0_SSIP = dfc.SSIP_adj <= 0 \n",
    "    mask_SSIP_1q = (dfc.SSIP_adj > 0) & (dfc.SSIP_adj <= 5500) \n",
    "    mask_SSIP_2q = (dfc.SSIP_adj > 5500) & (dfc.SSIP_adj <= 8000) \n",
    "    mask_SSIP_3q = (dfc.SSIP_adj > 8000)\n",
    "\n",
    "    categories = [mask_0_SSIP, mask_SSIP_1q, mask_SSIP_2q, mask_SSIP_3q, mask_any]\n",
    "    category_names = ['SSIP0', 'SSIP1q', 'SSIP2q', 'SSIP3q', 'SSIP_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'SSIP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with SSIP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for public assistance income (PA_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_0_PA = dfc.PA_adj <= 0 \n",
    "    mask_PA_1q = (dfc.PA_adj > 0) & (dfc.PA_adj <= 900) \n",
    "    mask_PA_2q = (dfc.PA_adj > 900)\n",
    "\n",
    "    categories = [mask_0_PA, mask_PA_1q, mask_PA_2q, mask_any]\n",
    "    category_names = ['PA0', 'PA1q', 'PA2q', 'PA_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'PA_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with PA')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for retirement income (RETP_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_RETP_1q = (dfc.RETP_adj > 0) & (dfc.RETP_adj <= 6000) \n",
    "    mask_RETP_2q = (dfc.RETP_adj > 6000) & (dfc.RETP_adj <= 13400) \n",
    "    mask_RETP_3q = (dfc.RETP_adj > 13400)\n",
    "\n",
    "    categories = [mask_RETP_1q, mask_RETP_2q, mask_RETP_3q, mask_any]\n",
    "    category_names = ['RETP1q', 'RETP2q', 'RETP3q', 'RETP_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'RETP_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with RETP')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for other income (OI_adj)\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    # cutoffs taken from quartiles of nonzero values\n",
    "    mask_OI_1q = (dfc.OI_adj > 0) & (dfc.OI_adj <= 2000) \n",
    "    mask_OI_2q = (dfc.OI_adj > 2000) & (dfc.OI_adj <= 6000) \n",
    "    mask_OI_3q = (dfc.OI_adj > 6000)\n",
    "\n",
    "    categories = [mask_OI_1q, mask_OI_2q, mask_OI_3q, mask_any]\n",
    "    category_names = ['OI1q', 'OI2q', 'OI3q', 'OI_any']\n",
    "\n",
    "    if include_financials:\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'OI_adj')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with OI')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    # add columns for ethnicity\n",
    "    groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "    group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "    mask_white = dfc.Ethnicity == 1\n",
    "    mask_black = dfc.Ethnicity == 2\n",
    "    mask_asian = dfc.Ethnicity == 3\n",
    "    mask_hisp = dfc.Ethnicity == 4\n",
    "    mask_other = dfc.Ethnicity == 5\n",
    "\n",
    "    categories = [mask_white, mask_black, mask_asian, mask_hisp, mask_other, mask_any]\n",
    "    category_names = ['White', 'Black', 'Asian', 'Hisp', 'ETH_other', 'ETH_any']\n",
    "\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'Ethnicity')\n",
    "    add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "    print('done with Ethnicity')\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s so far')\n",
    "\n",
    "    if include_financials:\n",
    "        # add columns for total personal income that we added above ('TINP')\n",
    "        groups = [mask_adult, mask_65_plus, mask_18_64, mask_kid, mask_any_age]\n",
    "        group_names = ['adult', '65+', '18-64', 'kid', 'anyage']\n",
    "\n",
    "        mask_0_TINP = dfc.TINP == 0\n",
    "        mask_u10_TINP = dfc.TINP < 10000\n",
    "        mask_u15_TINP = dfc.TINP < 15000\n",
    "        mask_u20_TINP = dfc.TINP < 20000\n",
    "        mask_u25_TINP = dfc.TINP < 25000\n",
    "        mask_u30_TINP = dfc.TINP < 30000\n",
    "        mask_u35_TINP = dfc.TINP < 35000\n",
    "        mask_u40_TINP = dfc.TINP < 40000\n",
    "        mask_u45_TINP = dfc.TINP < 45000\n",
    "        mask_u50_TINP = dfc.TINP < 50000\n",
    "        mask_u60_TINP = dfc.TINP < 60000\n",
    "        mask_u70_TINP = dfc.TINP < 70000\n",
    "        mask_u80_TINP = dfc.TINP < 80000\n",
    "\n",
    "        categories = [mask_0_TINP, mask_u10_TINP, mask_u15_TINP, mask_u20_TINP, mask_u25_TINP, mask_u30_TINP,  mask_u35_TINP, \n",
    "                      mask_u40_TINP, mask_u45_TINP, mask_u50_TINP, mask_u60_TINP, mask_u70_TINP, mask_u80_TINP, mask_any]\n",
    "        category_names = ['TINP0', 'TINP<10', 'TINP<15', 'TINP<20', 'TINP<25', 'TINP<30', 'TINP<35', \n",
    "                          'TINP<40', 'TINP<45', 'TINP<50', 'TINP<60', 'TINP<70', 'TINP<80', 'TINP_any']\n",
    "\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'TINP')\n",
    "        add_pu_columns(dfc, groups, group_names, categories, category_names, 'AGEP')\n",
    "\n",
    "    time_took = time.time() - time_0\n",
    "    print('took ' + str(time_took) + ' s')\n",
    "\n",
    "    # Only return the new features that we engineered\n",
    "    # The variables features_to_mask and max_ppl were created at the beginning of this function\n",
    "    columns_to_mask = features_to_mask * max_ppl\n",
    "    dfc = dfc.iloc[:, columns_to_mask:].copy()\n",
    "\n",
    "    # We ended up with multi-level column headers - just keep the top level\n",
    "    dfc.columns = dfc.columns.get_level_values(0)\n",
    "    \n",
    "    return(dfc)\n",
    "\n",
    "#new_features = engineer_features(all_2016, include_financials=True)\n",
    "\n",
    "#new_features.to_csv('data/EngineeredFeatures.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NYCgov_Income instead of NYCGov_Pov_Stat\n",
    "Since poverty status is essentially whether (Total Poverty-Unit Income) is less than (Poverty-Unit Threshold), and since\n",
    "the poverty-unit threshold is fixed for a given household, their poverty status boils down to their income. \n",
    "\n",
    "So, let's \n",
    "re-run the models against "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Putting Personal Features into Poverty Unit Rows\n",
    "Our dataset contains people in poverty units (a household may contain one or more poverty units). The entire \n",
    "poverty unit either is or is not in poverty, but the data set as constructed has people in different rows (the data is\n",
    "not tidy).\n",
    "\n",
    "To tidy up, we'll move information on all the people in the poverty unit, into the row for that poverty unit.\n",
    "Instead of having 3 people in a poverty unit represented by different rows, we'll put all three people in\n",
    "the same row but different columns. The columns will be named 'AGEP_1', 'AGEP_2', 'AGEP_3', etc, with zero values in all\n",
    "columns where person n does not exist.\n",
    "\n",
    "There are three main columns of interest for this:\n",
    "* SERIALNO is the serial number of each household.\n",
    "* PovUnit_ID is the serial number of the poverty unit within the household (1-18). Each household can have more than one poverty unit (although the vast majority of households have only one poverty unit).\n",
    "* SPORDER is the serial number of a person in the household (1-20). Note that the dataset only assigns serial numbers to\n",
    "the people in the *household*, not the *poverty unit*.  This means that if for example a household has two poverty units,\n",
    "the first with two people and the second with three people, the head of the second poverty unit will have SPORDER of 3,\n",
    "not 1.  (One-based counting scheme) This is not a problem, but a particularity to be aware of when looking at dataset\n",
    "rows for reference.\n",
    "\n",
    "Also, there are some poverty-unit-level columns (e.g. 'TotalWorkHrs_PU', the number of work hours in the poverty unit)\n",
    "that have the same value for each person in the poverty unit; we'll collect those separately.\n",
    "\n",
    "So the strategy in the next section is to create dataframes X_pers and X_pu, containing respectively the personal and\n",
    "poverty-unit features for each household.  We'll join those together, and then at the end add in all the new features we created in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pers_and_pu_features(df, include_financials=True, target_column='NYCgov_Pov_Stat'):\n",
    "    \"\"\"Create personal and poverty-unit features for the dataframe. No new features are created, just 'tidy'-ing the data.\n",
    "    Input: a poverty dataframe, whether or not to include financial features, and the target column.\n",
    "    Output: returns a copy of the dataframe, tidy-ed up, with poverty units in rows and only features of interest in \n",
    "    columns. Prints progess updates to the screen as it goes.\n",
    "    \"\"\"\n",
    "\n",
    "    dfc = df.copy()\n",
    "    \n",
    "    # First, some categoricals have odd ordering; remap them\n",
    "    fix_orders = {'ENG': {0:0, 4:1, 3:2, 2:3, 1:4, 5:5}, 'WKW': {0:0, 6:1, 5:2, 4:3, 3:4, 2:5, 1:6}, \n",
    "                  'TotalWorkHrs_PU': {5:0, 4:1, 3:2, 2:3, 1:4}}\n",
    "    dfc['ENG'] = dfc['ENG'].map(fix_orders['ENG'])\n",
    "    dfc['WKW'] = dfc['WKW'].map(fix_orders['WKW'])\n",
    "    dfc['TotalWorkHrs_PU'] = dfc['TotalWorkHrs_PU'].map(fix_orders['TotalWorkHrs_PU'])\n",
    "\n",
    "    # Add column for total personal income\n",
    "    if include_financials:\n",
    "        dfc['TINP'] = dfc.WAGP_adj + dfc.INTP_adj + dfc.SEMP_adj + dfc.SSP_adj + dfc.SSIP_adj + dfc.PA_adj + dfc.RETP_adj + dfc.OI_adj\n",
    "\n",
    "    categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', \n",
    "                    'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "    # We'll create separate dataframes for personal and poverty-unit variables, then join them together\n",
    "    personal_columns = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro']\n",
    "    if include_financials:\n",
    "        personal_columns += ['WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', \n",
    "                             'OI_adj', 'TINP']\n",
    "    pu_columns = ['NP', 'TEN', 'HHT', 'MRGP_adj', 'RNTP_adj', 'HousingStatus', 'TotalWorkHrs_PU'] + target_column\n",
    "\n",
    "    # Create a dataframe for the personal columns, including our 3 indicator variables\n",
    "    df_pers = dfc.copy()\n",
    "    df_pers_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + personal_columns\n",
    "    df_pers = df_pers[df_pers_columns]\n",
    "\n",
    "    # Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "    df_pers = df_pers.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "    # Turn the multi-index columns into a single indexed column: 'AGEP_1', 'AGEP_2', 'AGEP_3', etc.\n",
    "    df_pers.columns = list(map('_'.join, [(y, str(z)) for y, z in (x for x in df_pers.columns)]))\n",
    "    print('df_pers complete')\n",
    "\n",
    "    # Create a dataframe for the poverty-unit columns, including our 3 indicator variables\n",
    "    df_pu = dfc.copy()\n",
    "    df_pu_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + pu_columns\n",
    "    df_pu = df_pu[df_pu_columns]\n",
    "\n",
    "    # Add column for total mortgage + rent\n",
    "    df_pu['MRNT'] = df_pu.MRGP_adj + df_pu.RNTP_adj\n",
    "\n",
    "    # Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "    df_pu = df_pu.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "    # Groupby and take the max of SPORDER (these are poverty-unit variables; if there is a nonzero value, it's unique)\n",
    "    df_pu = df_pu.stack().groupby(['SERIALNO', 'Povunit_ID']).max()\n",
    "    print('df_pu complete')\n",
    "\n",
    "    # Add the personal and poverty-unit dataframes\n",
    "    dfc = df_pers.join(df_pu)\n",
    "    return(dfc)\n",
    "\n",
    "\n",
    "# Get the personal and poverty-unit features\n",
    "#X = pers_and_pu_features(all_2016, include_financials=True, target_column='NYCgov_Pov_Stat')\n",
    "\n",
    "# Add the personal and poverty-unit dataframes\n",
    "# new_features = pd.read_csv('/data/EngineeredFeatures.csv', index_col=[0,1], header=0)\n",
    "#X = X.join(new_features)\n",
    "    \n",
    "#X.to_csv('/data/Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/Features.csv', index_col=[0,1], header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Coding Categorical Variables for One-Hot Encoding\n",
    "We need to do a little cleanup on the data before one-hot encoding. Some of our columns are categorical, and we need\n",
    "them to have type Categorical before one-hot encoding.  \n",
    "\n",
    "Normally we could just pass this to a pre-processor like \n",
    "dask-ml's 'Categorizer'; but remember that we split up each column like 'SCHL' into 20 columns 'SCHL_1', 'SCHL_2', etc. \n",
    "Moreover, some of these columns, like 'SCHL_20', will be pretty sparse since there are very few households with 20\n",
    "people. So we want to be sure that when we code each 'SCHL_n' column, we have ALL of the possible SCHL values encoded in\n",
    "the Categorical so that we don't get an error when we try to generalize to unseen data.\n",
    "\n",
    "Some categories are unordered (e.g. disability\n",
    "status), and we have to be aware of personal categories vs poverty-unit categories; the poverty-unit categories only show\n",
    "up in one column each, while the personal categories show up in many columns each (suffixed with '\\_1', '\\_2', etc.).\n",
    "\n",
    "Again this is one-off code poorly disguised as a function, solely for DRY purposes. 'Tighter' code would be to pull out\n",
    "the hard-coded lists and turn the loops into helper functions - falling back on YAGNI and 3-strikes rule for now (we're\n",
    "at 2 strikes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_categoricals(df, old_df):\n",
    "    \"\"\"Turn the categorical columns of df into true Categorical types in preparation for one-hot encoding, and return a \n",
    "    dictionary of columns to pass to the one-hot encoder.\n",
    "    Input: the dataframe of interest, and the 'old' dataframe with the original categories. (The dataframe at this\n",
    "    point will include columns like 'AGEP_1', 'AGEP_2', etc. which may not have all of the values, so we refer to old_df.)\n",
    "    Output: returns a 2-tuple of the dataframe with columns transformed, and a dictionary to pass to the one-hot encoder.)\n",
    "    \"\"\"\n",
    "    \n",
    "    dfc = df.copy()\n",
    "    \n",
    "    # Number of enumerated columns for each feature ('AGEP_1', 'AGEP_2', etc.)\n",
    "    # This is equal to the maximum number of people in any household, which is the max of SPORDER\n",
    "    n = old_df.SPORDER.max()\n",
    "\n",
    "    # All the categoricals that we'll have to set up\n",
    "    categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "    personal_categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro']\n",
    "    pu_categoricals = ['NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "    categories = {} # Dict for each initial categorical\n",
    "\n",
    "    # Some categoricals have no ordering\n",
    "    unordered = ['DIS', 'SEX', 'MSP', 'JWTR', 'Ethnicity', 'Boro', 'TEN', 'HHT', 'HousingStatus']\n",
    "\n",
    "    # Loop through and assign appropriate category structure for personal categoricals\n",
    "    for feature in personal_categoricals:\n",
    "        cats = old_df[feature].unique() # Get all of the category values, to use in assigning the type of Categorical\n",
    "        if not 0 in cats:\n",
    "            # Even if 0 was not in the original categorization - here it means 'no person', so we need it\n",
    "            cats = np.append(cats, 0)\n",
    "        cats.sort()\n",
    "        # Loop through and assign for each suffixed column '_1', '_2', etc.\n",
    "        for i in range(1,n+1):\n",
    "            suffixed_name = feature + '_' + str(i)\n",
    "            # Assign Categorical type to columns\n",
    "            if feature in unordered:\n",
    "                categories[suffixed_name] = pd.Categorical(cats, ordered=False)\n",
    "                dfc[suffixed_name] = pd.Categorical(dfc[suffixed_name], ordered=False, categories=cats)\n",
    "            else: # Category is ordered\n",
    "                categories[suffixed_name] = pd.Categorical(cats, ordered=True, categories=cats)\n",
    "                dfc[suffixed_name] = pd.Categorical(dfc[suffixed_name], ordered=True, categories=cats)\n",
    "\n",
    "    # Loop through and assign appropriate category structure for poverty-unit categoricals\n",
    "    for feature in pu_categoricals:\n",
    "        cats = old_df[feature].unique() # Get all of the category values, to use in assigning the type of Categorical\n",
    "        if not 0 in cats:\n",
    "            # Even if 0 was not in the original categorization - here it means 'no person', so we need it\n",
    "            cats = np.append(cats, 0)\n",
    "        cats.sort()\n",
    "        # Assign Categorical type to columns\n",
    "        if feature in unordered:\n",
    "            categories[suffixed_name] = pd.Categorical(cats, ordered=False)\n",
    "            dfc[feature] = pd.Categorical(dfc[feature], ordered=False, categories=cats)\n",
    "        else: # Category is ordered\n",
    "            categories[suffixed_name] = pd.Categorical(cats, ordered=True, categories=cats)\n",
    "            dfc[feature] = pd.Categorical(dfc[feature], ordered=True, categories=cats)\n",
    "\n",
    "    # Create a dictionary 'dummy_these' that we'll pass to our dummy-maker later\n",
    "    # The poverty-unit categoricals can be passed as-is. For the personal categoricals, we'll have features \n",
    "    # like 'AGEP_1', 'AGEP_2', ..., 'AGEP_20' - so we have to loop through and assign categories.\n",
    "    dummy_these = {}\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        for feature in personal_categoricals:\n",
    "            name = feature + '_' + str(i)\n",
    "            dummy_these[name] = categories[name]\n",
    "\n",
    "    for feature in pu_categoricals:\n",
    "        dummy_these[feature] = categories[name]\n",
    "    \n",
    "    return(dfc, dummy_these)\n",
    "\n",
    "#X, dummy_these = code_categoricals(X, all_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('data/FeaturesCoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 0.88s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 32.85s to fit \n",
      "{'balanced_accuracy_score': 0.6904761904761905, 'geometric_mean_score': 0.6900655593423543, 'confusion_matrix': array([[2, 1],\n",
      "       [2, 5]]), 'classification_report': '                   pre       rec       spe        f1       geo       iba       sup\\n\\n        1.0       0.50      0.67      0.71      0.57      0.69      0.47         3\\n        2.0       0.83      0.71      0.67      0.77      0.69      0.48         7\\n\\navg / total       0.73      0.70      0.68      0.71      0.69      0.48        10\\n', 'feature_importances_': array([0., 0., 0., ..., 0., 0., 0.]), 'transformed_columns_': Index(['WAGP_adj_1', 'WAGP_adj_2', 'WAGP_adj_3', 'WAGP_adj_4', 'WAGP_adj_5',\n",
      "       'WAGP_adj_6', 'WAGP_adj_7', 'WAGP_adj_8', 'WAGP_adj_9', 'WAGP_adj_10',\n",
      "       ...\n",
      "       'HousingStatus_5', 'HousingStatus_6', 'HousingStatus_7',\n",
      "       'HousingStatus_8', 'HousingStatus_9', 'TotalWorkHrs_PU_1',\n",
      "       'TotalWorkHrs_PU_2', 'TotalWorkHrs_PU_3', 'TotalWorkHrs_PU_4',\n",
      "       'TotalWorkHrs_PU_5'],\n",
      "      dtype='object', length=10477)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 7.82s to fit \n",
      "{'balanced_accuracy_score': 0.6190476190476191, 'geometric_mean_score': 0.6172133998483675, 'confusion_matrix': array([[2, 1],\n",
      "       [3, 4]]), 'classification_report': '                   pre       rec       spe        f1       geo       iba       sup\\n\\n        1.0       0.40      0.67      0.57      0.50      0.62      0.38         3\\n        2.0       0.80      0.57      0.67      0.67      0.62      0.38         7\\n\\navg / total       0.68      0.60      0.64      0.62      0.62      0.38        10\\n', 'feature_importances_': array([0., 0., 0., ..., 0., 0., 0.]), 'transformed_columns_': Index(['WAGP_adj_1', 'WAGP_adj_2', 'WAGP_adj_3', 'WAGP_adj_4', 'WAGP_adj_5',\n",
      "       'WAGP_adj_6', 'WAGP_adj_7', 'WAGP_adj_8', 'WAGP_adj_9', 'WAGP_adj_10',\n",
      "       ...\n",
      "       'HousingStatus_5', 'HousingStatus_6', 'HousingStatus_7',\n",
      "       'HousingStatus_8', 'HousingStatus_9', 'TotalWorkHrs_PU_1',\n",
      "       'TotalWorkHrs_PU_2', 'TotalWorkHrs_PU_3', 'TotalWorkHrs_PU_4',\n",
      "       'TotalWorkHrs_PU_5'],\n",
      "      dtype='object', length=10477)}\n"
     ]
    }
   ],
   "source": [
    "# Take a small subset of the data to run POC\n",
    "X_small = X.iloc[:50, :].copy()\n",
    "\n",
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X_small['NYCgov_Pov_Stat']\n",
    "y.replace({1: 'Pov', 2:'Not Pov'}, inplace=True)\n",
    "X = X_small.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "#samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "scaler = Normalizer()\n",
    "clf =                  RandomForestClassifier(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "balanced_clf = BalancedRandomForestClassifier(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "# Create empty dictionaries to hold results\n",
    "results_plain = {}\n",
    "results_balanced = {}\n",
    "\n",
    "for classifier, results_dict in zip([clf, balanced_clf], [results_plain, results_balanced]):\n",
    "\n",
    "    pipeline = imbPipeline(steps=[#('cat', categorizer), # No need for Categorizer since we already did it\n",
    "                                  ('dummies', dummy_encoder), \n",
    "                                  #('sampler', sampler), \n",
    "                                  ('scaler', scaler), \n",
    "                                  ('clf', classifier)], \n",
    "                           memory=cachedir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    time_to_fit = time.time() - t0\n",
    "    print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    results_dict['balanced_accuracy_score'] = balanced_accuracy_score(y_test, predictions)\n",
    "    results_dict['geometric_mean_score'] = geometric_mean_score(y_test, predictions)\n",
    "    results_dict['confusion_matrix'] = confusion_matrix(y_test, predictions)\n",
    "    results_dict['classification_report'] = classification_report_imbalanced(y_test, predictions)\n",
    "    results_dict['feature_importances_'] = pipeline.named_steps['clf'].feature_importances_\n",
    "    results_dict['transformed_columns_'] = pipeline.named_steps['dummies'].transformed_columns_\n",
    "    print(str(results_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 'min_adult_any_SEX'),\n",
       " (0.1, 'all_adult_any_WKHP'),\n",
       " (0.1, 'max_adult_WAG<35'),\n",
       " (0.1, 'min_adult_WAG<45'),\n",
       " (0.1, 'count_anyage_WAG0'),\n",
       " (0.1, 'mean_anyage_SSIP_any'),\n",
       " (0.1, 'all_adult_TINP<20'),\n",
       " (0.09074074074074075, 'all_anyage_any_WKW'),\n",
       " (0.09034749034749033, 'min_adult_any_WKHP'),\n",
       " (0.07692307692307691, 'min_anyage_SSIP_any'),\n",
       " (0.023076923076923078, 'min_adult_not_DIS'),\n",
       " (0.009652509652509658, 'any_anyage_SSP_any'),\n",
       " (0.00925925925925926, 'any_18-64_nonzero_WKW'),\n",
       " (0.0, 'WAGP_adj_1'),\n",
       " (0.0, 'WAGP_adj_2')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = list(zip(results_balanced['feature_importances_'], results_balanced['transformed_columns_']))\n",
    "sorted(tester, key=lambda tup: tup[0], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - No Financials\n",
    "This will be as above, but pulling out the financial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with CIT\n",
      "took 177.80941009521484 s so far\n",
      "done with SCHL\n",
      "took 368.13257026672363 s so far\n",
      "done with SEX\n",
      "took 487.51518177986145 s so far\n",
      "done with ENG\n",
      "took 740.9991555213928 s so far\n",
      "done with MSP\n",
      "took 965.9160234928131 s so far\n",
      "done with WKW\n",
      "took 1456.2761499881744 s so far\n",
      "done with WKHP\n",
      "took 2032.8705401420593 s so far\n",
      "done with DIS\n",
      "took 2202.0311136245728 s so far\n",
      "done with NP\n",
      "took 2862.999502182007 s so far\n",
      "done with JWTR\n",
      "took 2918.9622707366943 s so far\n",
      "done with WAGP\n",
      "took 2919.08518576622 s so far\n",
      "done with INTP\n",
      "took 2919.1623170375824 s so far\n",
      "done with SEMP\n",
      "took 2919.2510974407196 s so far\n",
      "done with SSP\n",
      "took 2919.3353056907654 s so far\n",
      "done with SSIP\n",
      "took 2919.4014370441437 s so far\n",
      "done with PA\n",
      "took 2919.439707517624 s so far\n",
      "done with RETP\n",
      "took 2919.497378349304 s so far\n",
      "done with OI\n",
      "took 2919.558589220047 s so far\n",
      "done with Ethnicity\n",
      "took 3303.634963274002 s so far\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'TINP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-8a3875fe082d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_no_fin_new_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengineer_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_financials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_no_fin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpers_and_pu_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_2016\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_financials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NYCgov_Pov_Stat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_no_fin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_no_fin_new_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-17ac3c462eba>\u001b[0m in \u001b[0;36mengineer_features\u001b[0;34m(df, include_financials)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0mgroup_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'adult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'65+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'18-64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anyage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m     \u001b[0mmask_0_TINP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTINP\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m     \u001b[0mmask_u10_TINP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTINP\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0mmask_u15_TINP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTINP\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'TINP'"
     ]
    }
   ],
   "source": [
    "X_no_fin_new_features = engineer_features(all_2016, include_financials=False)\n",
    "X_no_fin = pers_and_pu_features(all_2016, include_financials=False, target_column='NYCgov_Pov_Stat')\n",
    "\n",
    "X_no_fin.join(X_no_fin_new_features)\n",
    "\n",
    "X_no_fin.to_csv('/data/FeaturesNoFin.csv')\n",
    "\n",
    "X_no_fin, dummy_these = code_categoricals(X_no_fin, all_2016)\n",
    "X_no_fin.to_csv('data/FeaturesNoFinCoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_fin = pd.read_csv('data/FeaturesNoFinCoded.csv', index_col=[0,1], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SERIALNO  Povunit_ID\n",
       "39        1             Not Pov\n",
       "55        1             Not Pov\n",
       "69        1             Not Pov\n",
       "210       1             Not Pov\n",
       "261       1             Not Pov\n",
       "Name: NYCgov_Pov_Stat, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester = y.head()\n",
    "#tester.replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}}, inplace=True)\n",
    "tester.replace({1: 'Pov', 2:'Not Pov'}, inplace=True)\n",
    "tester\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small subset of the data to run POC\n",
    "X_small = X_no_fin.iloc[:50, :].copy()\n",
    "\n",
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X_small['NYCgov_Pov_Stat']\n",
    "y.replace({1: 'Pov', 2:'Not Pov'}, inplace=True)\n",
    "X = X_small.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "#samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "scaler = Normalizer()\n",
    "clf =                  RandomForestClassifier(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "balanced_clf = BalancedRandomForestClassifier(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "# Create empty dictionaries to hold results\n",
    "results_plain_no_fin = {}\n",
    "results_balanced_no_fin = {}\n",
    "\n",
    "for classifier, results_dict in zip([clf, balanced_clf], [results_plain_no_fin, results_balanced_no_fin]):\n",
    "\n",
    "    pipeline = imbPipeline(steps=[#('cat', categorizer), # No need for Categorizer since we already did it\n",
    "                                  ('dummies', dummy_encoder), \n",
    "                                  #('sampler', sampler), \n",
    "                                  ('scaler', scaler), \n",
    "                                  ('clf', classifier)], \n",
    "                           memory=cachedir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    time_to_fit = time.time() - t0\n",
    "    print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "    \n",
    "    predictions = pipeline.predict(X_test)\n",
    "    results_dict['balanced_accuracy_score'] = balanced_accuracy_score(y_test, predictions)\n",
    "    results_dict['geometric_mean_score'] = geometric_mean_score(y_test, predictions)\n",
    "    results_dict['confusion_matrix'] = confusion_matrix(y_test, predictions)\n",
    "    results_dict['classification_report'] = classification_report_imbalanced(y_test, predictions)\n",
    "    results_dict['feature_importances_'] = pipeline.named_steps['clf'].feature_importances_\n",
    "    results_dict['transformed_columns_'] = pipeline.named_steps['dummies'].transformed_columns_\n",
    "    print(str(results_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = list(zip(results_balanced_no_fin['feature_importances_'], results_balanced_no_fin['transformed_columns_']))\n",
    "sorted(tester, key=lambda tup: tup[0], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NYCgov_Pov_Gap instead of NYCGov_Pov_Stat\n",
    "We've been looking at poverty status as a classification problem (yes or no), but we can also view it as a regression \n",
    "problem.  Poverty status is essentially whether (Total Poverty-Unit Income) is less than (Poverty-Unit Threshold); but\n",
    "rather than viewing that as a yes-no, the data set has a feature 'NYCgov_Pov_Gap' which is, for households in poverty, \n",
    "the difference between the two.\n",
    "\n",
    "So, let's run the models against 'NYCgov_Pov_Gap' to see where that gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pg_ef = pd.read_csv('data/EngineeredFeatures.csv', index_col=[0,1], header=0) # Reuse existing; no change here\n",
    "X_pg = pers_and_pu_features(all_2016, include_financials=True, target_column='NYCgov_Pov_Gap')\n",
    "\n",
    "X_pg.join(X_pg_ef)\n",
    "\n",
    "X_pg.to_csv('/data/PGFeatures.csv')\n",
    "\n",
    "X_pg, dummy_these = code_categoricals(X_pg, all_2016)\n",
    "X_pg.to_csv('data/PGFeaturesCoded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small subset of the data to run POC\n",
    "X_small = X_pg.iloc[:50, :].copy()\n",
    "\n",
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X_small['NYCgov_Pov_Gap']\n",
    "X = X_small.drop('NYCgov_Pov_Gap', axis='columns')\n",
    "\n",
    "# Get train and test - no stratifying here since we're doing regression, not classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "scaler = Normalizer()\n",
    "clf =                  RandomForestRegressor(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "balanced_clf = BalancedRandomForestRegressor(n_jobs=-1, n_estimators=10, max_features='auto', random_state=42)\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "# Create empty dictionaries to hold results\n",
    "results_plain_no_fin = {}\n",
    "results_balanced_no_fin = {}\n",
    "\n",
    "for classifier, results_dict in zip([clf, balanced_clf], [results_plain_no_fin, results_balanced_no_fin]):\n",
    "\n",
    "    pipeline = imbPipeline(steps=[#('cat', categorizer), # No need for Categorizer since we already did it\n",
    "                                  ('dummies', dummy_encoder), \n",
    "                                  ('scaler', scaler), \n",
    "                                  ('clf', classifier)], \n",
    "                           memory=cachedir)\n",
    "\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    time_to_fit = time.time() - t0\n",
    "    print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "    \n",
    "    results_dict['training score'] = pipeline.named_steps['clf'].score(X_train, y_train)\n",
    "    results_dict['test score'] = pipeline.named_steps['clf'].score(X_test, y_test)\n",
    "    results_dict['feature_importances_'] = pipeline.named_steps['clf'].feature_importances_\n",
    "    results_dict['transformed_columns_'] = pipeline.named_steps['dummies'].transformed_columns_\n",
    "    print(str(results_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "Old stuff that I'm currently leaving here as an appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X['NYCgov_Pov_Stat'].replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}})\n",
    "X = X.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data with sampling strategies (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "# 5) classifiers\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "#scalers = [StandardScaler(), Normalizer(), PowerTransformer(), QuantileTransformer()]\n",
    "scalers = [Normalizer()]\n",
    "scaler = Normalizer()\n",
    "#classifiers = [LogisticRegression(), SGDClassifier(), AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), \n",
    "               #RandomForestClassifier(), BalancedBaggingClassifier(), BalancedRandomForestClassifier(), RUSBoostClassifier()]\n",
    "#classifiers = [BalancedBaggingClassifier(), BaggingClassifier(), RandomForestClassifier(), BalancedRandomForestClassifier(), \n",
    "               #AdaBoostClassifier(), GradientBoostingClassifier()]\n",
    "#classifiers = [RandomForestClassifier(), BalancedRandomForestClassifier()]\n",
    "classifiers = [BalancedRandomForestClassifier()]\n",
    "\n",
    "#sampler = TomekLinks(random_state=42)\n",
    "#scaler = QuantileTransformer()\n",
    "#clf = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "#clf = RandomForestClassifier(n_estimators=100)\n",
    "#clf = AdaBoostClassifier()\n",
    "#params={0: {'clf__C': [1, 1e-1, 1e-2, 1e-3], 'clf__max_iter': [1e2, 1e3, 1e4], # Logistic Regression\n",
    "                               #'clf__solver': ['lbfgs', 'liblinear', 'sag', 'saga']}, \n",
    "        #1: {'n_estimators': [1e1, 1e2, 1e3], 'max_features': [5, 10, 50, 100], # Random Forest Classifier\n",
    "                         #'criterion': ['gini', 'entropy']}\n",
    "       #}\n",
    "#params = {0: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy']},\n",
    "          #1: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy'], 'clf_sampling_strategy': [0.05, 0.25, 0.5, 0.75, 0.95]}\n",
    "         #}\n",
    "\n",
    "params = {0: {'clf__n_estimators': [1000], 'clf__max_features': [100],\n",
    "              'clf__sampling_strategy': ['not minority', 'not majority', 'all']}}\n",
    "          #1: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy']},\n",
    "         #}\n",
    "\n",
    "#parameters = {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100], 'clf__criterion': ['gini', 'entropy']}\n",
    "parameters = {'clf__n_estimators': [100], 'clf__max_features': ['auto'], 'clf__criterion': ['gini']}\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "#pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                              #('dummies', dummy_encoder),\n",
    "                              #('sampler', sampler),\n",
    "                              #('scaler', scaler),\n",
    "                              #('clf', BalancedRandomForestClassifier())], \n",
    "                      #memory=cachedir)\n",
    "                    \n",
    "#grid = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=5, n_jobs=-1, pre_dispatch=2, verbose=9)#, scoring=balanced_accuracy_score())\n",
    "#grid = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=5, n_jobs=-1, verbose=9)\n",
    "\n",
    "#t0 = time.time()\n",
    "#grid.fit(X_train, y_train)\n",
    "#time_to_fit = time.time() - t0\n",
    "#print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "#print(grid.cv_results_)\n",
    "\n",
    "#for sampler, i in zip(samplers, range(len(samplers))):\n",
    "for i in range(1):\n",
    "    #for scaler, j in zip(scalers, range(len(scalers))):\n",
    "    for scaler in scalers:\n",
    "        for k in range(len(classifiers)):\n",
    "            #pipeline = Pipeline(steps=[#('cat', categorizer),\n",
    "            pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                                          ('dummies', dummy_encoder),\n",
    "                                          #('sampler', sampler),\n",
    "                                          ('scaler', scaler),\n",
    "                                          ('clf', classifiers[k])],\n",
    "                                  memory=cachedir)\n",
    "\n",
    "            #print(pipeline)\n",
    "            #print(params[i])\n",
    "            #pipeline.get_params().keys()\n",
    "            grid = GridSearchCV(estimator=pipeline, param_grid=params[k], cv=3, n_jobs=-1, verbose=9)#, scoring=balanced_accuracy_score())\n",
    "\n",
    "            t0 = time.time()\n",
    "            #pipeline.fit(X_train, y_train)\n",
    "            grid.fit(X_train, y_train)\n",
    "            time_to_fit = time.time() - t0\n",
    "            print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "            print(grid.cv_results_)\n",
    "            print('best estimator: ' + str(grid.best_estimator_))\n",
    "            print('best params: ' + str(grid.best_params_))\n",
    "            print('best index: ' + str(grid.best_index_))\n",
    "            \n",
    "            #print(str(sampler) + ',' + str(scaler) + ',' + str(classifiers[k]))\n",
    "            #print(str(scaler) + ',' + str(classifiers[k]))\n",
    "                  \n",
    "            #means = grid.cv_results_['mean_test_score']\n",
    "            #stds = grid.cv_results_['std_test_score']\n",
    "            #for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "                #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "            #predictions = pipeline.predict(X_test)\n",
    "\n",
    "#print('Predictions: ' + str(predictions))\n",
    "#print('Actual:\\n' + str(y_small))\n",
    "            #print('\\nBalanced accuracy: ' + str(balanced_accuracy_score(y_test, predictions)))\n",
    "            #print('Geometric mean: ' + str(geometric_mean_score(y_test, predictions)))\n",
    "            #print('Confusion matrix:\\n' + str(confusion_matrix(y_test, predictions)))\n",
    "            #print('\\nClassification report:\\n' + str(classification_report_imbalanced(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975358685378098"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometric_mean_score(grid.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.019535791275811728, 'WKW_2'),\n",
       " (0.020252231346152935, 'INTP_adj_1'),\n",
       " (0.020452137721584687, 'SCHL_2'),\n",
       " (0.020767468686063252, 'RNTP_adj'),\n",
       " (0.02515045254416771, 'WKHP_2'),\n",
       " (0.026205088630138082, 'AGEP_1'),\n",
       " (0.02817688155652038, 'SCHL_1'),\n",
       " (0.03238306328525264, 'WKW_1'),\n",
       " (0.03242418603775719, 'JWTR_1'),\n",
       " (0.03587014309356664, 'RETP_adj_1'),\n",
       " (0.041251391667779504, 'WKHP_1'),\n",
       " (0.05057339662820781, 'SSP_adj_1'),\n",
       " (0.052182437399624144, 'WAGP_adj_2'),\n",
       " (0.07156076688876317, 'TotalWorkHrs_PU'),\n",
       " (0.14873418922209963, 'WAGP_adj_1')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tester = BalancedRandomForestClassifier().fit(X=X_train, y=y_train)\n",
    "#len(tester.feature_importances_)\n",
    "#X_train.columns\n",
    "#geometric_mean_score(tester.predict(X_test), y_test)  #0.775727880752169\n",
    "imps = list(zip(tester.feature_importances_, X_train.columns))\n",
    "sorted(imps, key=lambda tup: tup[0])[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <th>param_clf__max_features</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.356531</td>\n",
       "      <td>0.774929</td>\n",
       "      <td>7.338355</td>\n",
       "      <td>0.130668</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.912454</td>\n",
       "      <td>0.913203</td>\n",
       "      <td>0.911165</td>\n",
       "      <td>0.912274</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>22</td>\n",
       "      <td>0.997085</td>\n",
       "      <td>0.995628</td>\n",
       "      <td>0.996769</td>\n",
       "      <td>0.996494</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.269967</td>\n",
       "      <td>2.907710</td>\n",
       "      <td>8.925081</td>\n",
       "      <td>2.030989</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.918662</td>\n",
       "      <td>0.920046</td>\n",
       "      <td>0.921176</td>\n",
       "      <td>0.919961</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.631243</td>\n",
       "      <td>11.630676</td>\n",
       "      <td>24.091638</td>\n",
       "      <td>2.337423</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.919422</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>0.919909</td>\n",
       "      <td>0.919919</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.915396</td>\n",
       "      <td>0.285015</td>\n",
       "      <td>8.093561</td>\n",
       "      <td>0.456263</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.911809</td>\n",
       "      <td>0.910404</td>\n",
       "      <td>0.910416</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>24</td>\n",
       "      <td>0.996515</td>\n",
       "      <td>0.996642</td>\n",
       "      <td>0.996706</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.711045</td>\n",
       "      <td>0.981712</td>\n",
       "      <td>7.328075</td>\n",
       "      <td>1.507757</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.916255</td>\n",
       "      <td>0.920806</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.919116</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151.528578</td>\n",
       "      <td>11.447976</td>\n",
       "      <td>19.518697</td>\n",
       "      <td>3.485998</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.919802</td>\n",
       "      <td>0.919792</td>\n",
       "      <td>0.921810</td>\n",
       "      <td>0.920468</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.778635</td>\n",
       "      <td>0.298928</td>\n",
       "      <td>7.595569</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.918388</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>18</td>\n",
       "      <td>0.995248</td>\n",
       "      <td>0.995628</td>\n",
       "      <td>0.996326</td>\n",
       "      <td>0.995734</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36.483098</td>\n",
       "      <td>3.360073</td>\n",
       "      <td>6.941224</td>\n",
       "      <td>1.106543</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.922463</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.925231</td>\n",
       "      <td>0.924016</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>353.551459</td>\n",
       "      <td>24.645494</td>\n",
       "      <td>14.723334</td>\n",
       "      <td>2.187714</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.922970</td>\n",
       "      <td>0.925494</td>\n",
       "      <td>0.927005</td>\n",
       "      <td>0.925156</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.911089</td>\n",
       "      <td>0.593627</td>\n",
       "      <td>7.788339</td>\n",
       "      <td>0.047878</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.914608</td>\n",
       "      <td>0.914090</td>\n",
       "      <td>0.917754</td>\n",
       "      <td>0.915484</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>19</td>\n",
       "      <td>0.995818</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996896</td>\n",
       "      <td>0.996304</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62.250116</td>\n",
       "      <td>6.208748</td>\n",
       "      <td>7.382807</td>\n",
       "      <td>1.209777</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.927024</td>\n",
       "      <td>0.929676</td>\n",
       "      <td>0.925992</td>\n",
       "      <td>0.927564</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>604.099197</td>\n",
       "      <td>40.388460</td>\n",
       "      <td>15.118818</td>\n",
       "      <td>2.104008</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.927277</td>\n",
       "      <td>0.927902</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.927690</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.635937</td>\n",
       "      <td>0.202809</td>\n",
       "      <td>8.041993</td>\n",
       "      <td>0.094281</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.911187</td>\n",
       "      <td>0.911049</td>\n",
       "      <td>0.910784</td>\n",
       "      <td>0.911007</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>23</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>0.995882</td>\n",
       "      <td>0.995882</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.703423</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>9.826621</td>\n",
       "      <td>0.189647</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.920182</td>\n",
       "      <td>0.919539</td>\n",
       "      <td>0.921049</td>\n",
       "      <td>0.920257</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>125.815334</td>\n",
       "      <td>10.479840</td>\n",
       "      <td>25.741467</td>\n",
       "      <td>3.183750</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.919676</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.920130</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.241193</td>\n",
       "      <td>0.842785</td>\n",
       "      <td>6.291520</td>\n",
       "      <td>1.195959</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.913088</td>\n",
       "      <td>0.909275</td>\n",
       "      <td>0.915347</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>21</td>\n",
       "      <td>0.996452</td>\n",
       "      <td>0.996642</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>0.996389</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.362928</td>\n",
       "      <td>1.091575</td>\n",
       "      <td>8.315560</td>\n",
       "      <td>0.948133</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.920182</td>\n",
       "      <td>0.921946</td>\n",
       "      <td>0.922443</td>\n",
       "      <td>0.921524</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>141.452599</td>\n",
       "      <td>12.096369</td>\n",
       "      <td>18.503021</td>\n",
       "      <td>1.951633</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.921196</td>\n",
       "      <td>0.921820</td>\n",
       "      <td>0.920542</td>\n",
       "      <td>0.921186</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.892606</td>\n",
       "      <td>0.185923</td>\n",
       "      <td>7.927964</td>\n",
       "      <td>0.195101</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.914354</td>\n",
       "      <td>0.915864</td>\n",
       "      <td>0.914713</td>\n",
       "      <td>0.914977</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>20</td>\n",
       "      <td>0.996262</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.995924</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28.135110</td>\n",
       "      <td>1.850436</td>\n",
       "      <td>7.699444</td>\n",
       "      <td>1.007361</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.925250</td>\n",
       "      <td>0.926635</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.925621</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>273.912406</td>\n",
       "      <td>17.282531</td>\n",
       "      <td>13.551803</td>\n",
       "      <td>1.713333</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.927151</td>\n",
       "      <td>0.927015</td>\n",
       "      <td>0.927386</td>\n",
       "      <td>0.927184</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.285676</td>\n",
       "      <td>0.373293</td>\n",
       "      <td>7.967199</td>\n",
       "      <td>0.167094</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.920172</td>\n",
       "      <td>0.919402</td>\n",
       "      <td>0.918525</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>17</td>\n",
       "      <td>0.997466</td>\n",
       "      <td>0.996009</td>\n",
       "      <td>0.995755</td>\n",
       "      <td>0.996410</td>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.109372</td>\n",
       "      <td>4.389925</td>\n",
       "      <td>7.425203</td>\n",
       "      <td>1.210115</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.926390</td>\n",
       "      <td>0.929042</td>\n",
       "      <td>0.926879</td>\n",
       "      <td>0.927437</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>356.878639</td>\n",
       "      <td>99.027548</td>\n",
       "      <td>11.762577</td>\n",
       "      <td>2.313773</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.928544</td>\n",
       "      <td>0.931323</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       29.356531      0.774929         7.338355        0.130668   \n",
       "1       21.269967      2.907710         8.925081        2.030989   \n",
       "2      133.631243     11.630676        24.091638        2.337423   \n",
       "3        6.915396      0.285015         8.093561        0.456263   \n",
       "4       17.711045      0.981712         7.328075        1.507757   \n",
       "5      151.528578     11.447976        19.518697        3.485998   \n",
       "6        8.778635      0.298928         7.595569        0.218479   \n",
       "7       36.483098      3.360073         6.941224        1.106543   \n",
       "8      353.551459     24.645494        14.723334        2.187714   \n",
       "9       10.911089      0.593627         7.788339        0.047878   \n",
       "10      62.250116      6.208748         7.382807        1.209777   \n",
       "11     604.099197     40.388460        15.118818        2.104008   \n",
       "12       6.635937      0.202809         8.041993        0.094281   \n",
       "13      18.703423      0.274799         9.826621        0.189647   \n",
       "14     125.815334     10.479840        25.741467        3.183750   \n",
       "15       5.241193      0.842785         6.291520        1.195959   \n",
       "16      16.362928      1.091575         8.315560        0.948133   \n",
       "17     141.452599     12.096369        18.503021        1.951633   \n",
       "18       7.892606      0.185923         7.927964        0.195101   \n",
       "19      28.135110      1.850436         7.699444        1.007361   \n",
       "20     273.912406     17.282531        13.551803        1.713333   \n",
       "21       9.285676      0.373293         7.967199        0.167094   \n",
       "22      44.109372      4.389925         7.425203        1.210115   \n",
       "23     356.878639     99.027548        11.762577        2.313773   \n",
       "\n",
       "   param_clf__criterion param_clf__max_features param_clf__n_estimators  \\\n",
       "0                  gini                       5                      10   \n",
       "1                  gini                       5                     100   \n",
       "2                  gini                       5                    1000   \n",
       "3                  gini                      10                      10   \n",
       "4                  gini                      10                     100   \n",
       "5                  gini                      10                    1000   \n",
       "6                  gini                      50                      10   \n",
       "7                  gini                      50                     100   \n",
       "8                  gini                      50                    1000   \n",
       "9                  gini                     100                      10   \n",
       "10                 gini                     100                     100   \n",
       "11                 gini                     100                    1000   \n",
       "12              entropy                       5                      10   \n",
       "13              entropy                       5                     100   \n",
       "14              entropy                       5                    1000   \n",
       "15              entropy                      10                      10   \n",
       "16              entropy                      10                     100   \n",
       "17              entropy                      10                    1000   \n",
       "18              entropy                      50                      10   \n",
       "19              entropy                      50                     100   \n",
       "20              entropy                      50                    1000   \n",
       "21              entropy                     100                      10   \n",
       "22              entropy                     100                     100   \n",
       "23              entropy                     100                    1000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'clf__criterion': 'gini', 'clf__max_features'...           0.912454   \n",
       "1   {'clf__criterion': 'gini', 'clf__max_features'...           0.918662   \n",
       "2   {'clf__criterion': 'gini', 'clf__max_features'...           0.919422   \n",
       "3   {'clf__criterion': 'gini', 'clf__max_features'...           0.909033   \n",
       "4   {'clf__criterion': 'gini', 'clf__max_features'...           0.916255   \n",
       "5   {'clf__criterion': 'gini', 'clf__max_features'...           0.919802   \n",
       "6   {'clf__criterion': 'gini', 'clf__max_features'...           0.916002   \n",
       "7   {'clf__criterion': 'gini', 'clf__max_features'...           0.922463   \n",
       "8   {'clf__criterion': 'gini', 'clf__max_features'...           0.922970   \n",
       "9   {'clf__criterion': 'gini', 'clf__max_features'...           0.914608   \n",
       "10  {'clf__criterion': 'gini', 'clf__max_features'...           0.927024   \n",
       "11  {'clf__criterion': 'gini', 'clf__max_features'...           0.927277   \n",
       "12  {'clf__criterion': 'entropy', 'clf__max_featur...           0.911187   \n",
       "13  {'clf__criterion': 'entropy', 'clf__max_featur...           0.920182   \n",
       "14  {'clf__criterion': 'entropy', 'clf__max_featur...           0.919676   \n",
       "15  {'clf__criterion': 'entropy', 'clf__max_featur...           0.913088   \n",
       "16  {'clf__criterion': 'entropy', 'clf__max_featur...           0.920182   \n",
       "17  {'clf__criterion': 'entropy', 'clf__max_featur...           0.921196   \n",
       "18  {'clf__criterion': 'entropy', 'clf__max_featur...           0.914354   \n",
       "19  {'clf__criterion': 'entropy', 'clf__max_featur...           0.925250   \n",
       "20  {'clf__criterion': 'entropy', 'clf__max_featur...           0.927151   \n",
       "21  {'clf__criterion': 'entropy', 'clf__max_featur...           0.916002   \n",
       "22  {'clf__criterion': 'entropy', 'clf__max_featur...           0.926390   \n",
       "23  {'clf__criterion': 'entropy', 'clf__max_featur...           0.928544   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.913203           0.911165         0.912274        0.000842   \n",
       "1            0.920046           0.921176         0.919961        0.001028   \n",
       "2            0.920426           0.919909         0.919919        0.000410   \n",
       "3            0.911809           0.910404         0.910416        0.001133   \n",
       "4            0.920806           0.920289         0.919116        0.002035   \n",
       "5            0.919792           0.921810         0.920468        0.000949   \n",
       "6            0.917511           0.918388         0.917300        0.000986   \n",
       "7            0.924354           0.925231         0.924016        0.001155   \n",
       "8            0.925494           0.927005         0.925156        0.001665   \n",
       "9            0.914090           0.917754         0.915484        0.001619   \n",
       "10           0.929676           0.925992         0.927564        0.001552   \n",
       "11           0.927902           0.927893         0.927690        0.000292   \n",
       "12           0.911049           0.910784         0.911007        0.000167   \n",
       "13           0.919539           0.921049         0.920257        0.000619   \n",
       "14           0.920426           0.920289         0.920130        0.000326   \n",
       "15           0.909275           0.915347         0.912570        0.002505   \n",
       "16           0.921946           0.922443         0.921524        0.000970   \n",
       "17           0.921820           0.920542         0.921186        0.000521   \n",
       "18           0.915864           0.914713         0.914977        0.000644   \n",
       "19           0.926635           0.924978         0.925621        0.000725   \n",
       "20           0.927015           0.927386         0.927184        0.000153   \n",
       "21           0.920172           0.919402         0.918525        0.001812   \n",
       "22           0.929042           0.926879         0.927437        0.001152   \n",
       "23           0.931323           0.927132         0.929000        0.001741   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                22            0.997085            0.995628   \n",
       "1                14            1.000000            1.000000   \n",
       "2                15            1.000000            1.000000   \n",
       "3                24            0.996515            0.996642   \n",
       "4                16            1.000000            1.000000   \n",
       "5                11            1.000000            1.000000   \n",
       "6                18            0.995248            0.995628   \n",
       "7                 8            1.000000            1.000000   \n",
       "8                 7            1.000000            1.000000   \n",
       "9                19            0.995818            0.996199   \n",
       "10                3            1.000000            1.000000   \n",
       "11                2            1.000000            1.000000   \n",
       "12               23            0.995311            0.995882   \n",
       "13               12            1.000000            1.000000   \n",
       "14               13            1.000000            1.000000   \n",
       "15               21            0.996452            0.996642   \n",
       "16                9            1.000000            1.000000   \n",
       "17               10            1.000000            1.000000   \n",
       "18               20            0.996262            0.995819   \n",
       "19                6            1.000000            1.000000   \n",
       "20                5            1.000000            1.000000   \n",
       "21               17            0.997466            0.996009   \n",
       "22                4            1.000000            1.000000   \n",
       "23                1            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "0             0.996769          0.996494         0.000626  \n",
       "1             1.000000          1.000000         0.000000  \n",
       "2             1.000000          1.000000         0.000000  \n",
       "3             0.996706          0.996621         0.000079  \n",
       "4             1.000000          1.000000         0.000000  \n",
       "5             1.000000          1.000000         0.000000  \n",
       "6             0.996326          0.995734         0.000446  \n",
       "7             1.000000          1.000000         0.000000  \n",
       "8             1.000000          1.000000         0.000000  \n",
       "9             0.996896          0.996304         0.000446  \n",
       "10            1.000000          1.000000         0.000000  \n",
       "11            1.000000          1.000000         0.000000  \n",
       "12            0.995882          0.995692         0.000269  \n",
       "13            1.000000          1.000000         0.000000  \n",
       "14            1.000000          1.000000         0.000000  \n",
       "15            0.996072          0.996389         0.000237  \n",
       "16            0.999937          0.999979         0.000030  \n",
       "17            1.000000          1.000000         0.000000  \n",
       "18            0.995692          0.995924         0.000244  \n",
       "19            1.000000          1.000000         0.000000  \n",
       "20            1.000000          1.000000         0.000000  \n",
       "21            0.995755          0.996410         0.000754  \n",
       "22            1.000000          1.000000         0.000000  \n",
       "23            1.000000          1.000000         0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'mean_fit_time': np.array([ 29.3565313 ,  21.269967  , 133.63124331,   6.9153959 ,\n",
    "        17.71104503, 151.52857844,   8.77863534,  36.48309787,\n",
    "       353.55145939,  10.91108894,  62.25011571, 604.09919691,\n",
    "         6.63593658,  18.70342271, 125.81533424,   5.24119258,\n",
    "        16.36292839, 141.45259889,   7.89260626,  28.13510966,\n",
    "       273.91240621,   9.28567576,  44.10937214, 356.8786389 ]), 'std_fit_time': np.array([ 0.77492853,  2.90770999, 11.63067571,  0.2850153 ,  0.98171153,\n",
    "       11.44797585,  0.29892795,  3.36007335, 24.64549427,  0.59362711,\n",
    "        6.20874779, 40.38846034,  0.20280898,  0.27479866, 10.47984029,\n",
    "        0.84278515,  1.09157463, 12.09636917,  0.18592348,  1.8504363 ,\n",
    "       17.28253135,  0.37329269,  4.38992519, 99.02754836]), 'mean_score_time': np.array([ 7.33835514,  8.92508117, 24.09163777,  8.09356125,  7.32807509,\n",
    "       19.51869694,  7.59556937,  6.94122378, 14.72333399,  7.78833922,\n",
    "        7.38280678, 15.11881781,  8.04199298,  9.8266205 , 25.7414674 ,\n",
    "        6.29152044,  8.31556026, 18.50302108,  7.92796397,  7.69944366,\n",
    "       13.55180319,  7.96719853,  7.42520293, 11.7625773 ]), 'std_score_time': np.array([0.1306685 , 2.03098883, 2.33742256, 0.45626301, 1.50775737,\n",
    "       3.48599838, 0.21847926, 1.10654311, 2.18771397, 0.04787844,\n",
    "       1.2097773 , 2.10400838, 0.09428107, 0.18964651, 3.18375043,\n",
    "       1.19595878, 0.94813268, 1.95163257, 0.19510065, 1.0073612 ,\n",
    "       1.71333338, 0.16709411, 1.21011483, 2.31377346]), 'param_clf__criterion': np.ma.masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
    "                   'gini', 'gini', 'gini', 'gini', 'gini', 'entropy',\n",
    "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
    "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
    "                   'entropy'],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'param_clf__max_features': np.ma.masked_array(data=[5, 5, 5, 10, 10, 10, 50, 50, 50, 100, 100, 100, 5, 5,\n",
    "                   5, 10, 10, 10, 50, 50, 50, 100, 100, 100],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'param_clf__n_estimators': np.ma.masked_array(data=[10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
    "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
    "                   100, 1000],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'params': [{'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 1000}], 'split0_test_score': np.array([0.91245407, 0.91866211, 0.91942227, 0.90903332, 0.91625491,\n",
    "       0.91980236, 0.91600152, 0.92246294, 0.92296972, 0.91460788,\n",
    "       0.92702395, 0.92727733, 0.91118713, 0.92018244, 0.91967566,\n",
    "       0.91308755, 0.92018244, 0.921196  , 0.91435449, 0.92525022,\n",
    "       0.92715064, 0.91600152, 0.92639047, 0.92854428]), 'split1_test_score': np.array([0.91320324, 0.92004562, 0.92042575, 0.91180943, 0.92080588,\n",
    "       0.91979219, 0.9175114 , 0.92435378, 0.92549417, 0.91409022,\n",
    "       0.92967562, 0.92790167, 0.91104916, 0.91953877, 0.92042575,\n",
    "       0.90927522, 0.92194627, 0.92181956, 0.91586417, 0.92663457,\n",
    "       0.9270147 , 0.92017233, 0.92904207, 0.93132286]), 'split2_test_score': np.array([0.91116462, 0.92117602, 0.91990876, 0.91040426, 0.92028894,\n",
    "       0.92180966, 0.91838804, 0.92523128, 0.92700545, 0.9177544 ,\n",
    "       0.92599164, 0.92789254, 0.91078444, 0.9210493 , 0.92028894,\n",
    "       0.9153466 , 0.92244329, 0.92054239, 0.91471296, 0.92497782,\n",
    "       0.92738563, 0.91940185, 0.92687872, 0.92713218]), 'mean_test_score': np.array([0.91227403, 0.91996114, 0.91991891, 0.91041561, 0.9191164 ,\n",
    "       0.92046798, 0.91730022, 0.92401588, 0.92515628, 0.91548403,\n",
    "       0.92756378, 0.92769049, 0.91100693, 0.9202568 , 0.92013009,\n",
    "       0.91256969, 0.92152391, 0.92118601, 0.91497719, 0.92562088,\n",
    "       0.92718365, 0.91852509, 0.92743707, 0.92899983]), 'std_test_score': np.array([0.00084192, 0.00102804, 0.00040974, 0.00113341, 0.00203455,\n",
    "       0.00094862, 0.00098567, 0.00115515, 0.00166481, 0.00161909,\n",
    "       0.00155163, 0.0002922 , 0.00016709, 0.00061889, 0.00032618,\n",
    "       0.00250546, 0.00097011, 0.00052144, 0.00064403, 0.00072536,\n",
    "       0.00015322, 0.00181211, 0.00115228, 0.00174085]), 'rank_test_score': np.array([22, 14, 15, 24, 16, 11, 18,  8,  7, 19,  3,  2, 23, 12, 13, 21,  9,\n",
    "       10, 20,  6,  5, 17,  4,  1], dtype=np.int32), 'split0_train_score': np.array([0.99708547, 1.        , 1.        , 0.99651524, 1.        ,\n",
    "       1.        , 0.99524805, 1.        , 1.        , 0.99581829,\n",
    "       1.        , 1.        , 0.99531141, 1.        , 1.        ,\n",
    "       0.99645188, 1.        , 1.        , 0.9962618 , 1.        ,\n",
    "       1.        , 0.99746563, 1.        , 1.        ]), 'split1_train_score': np.array([0.99562848, 1.        , 1.        , 0.99664217, 1.        ,\n",
    "       1.        , 0.99562848, 1.        , 1.        , 0.99619868,\n",
    "       1.        , 1.        , 0.99588191, 1.        , 1.        ,\n",
    "       0.99664217, 1.        , 1.        , 0.99581855, 1.        ,\n",
    "       1.        , 0.99600862, 1.        , 1.        ]), 'split2_train_score': np.array([0.99676908, 1.        , 1.        , 0.99670573, 1.        ,\n",
    "       1.        , 0.99632563, 1.        , 1.        , 0.99689579,\n",
    "       1.        , 1.        , 0.99588217, 1.        , 1.        ,\n",
    "       0.99607222, 0.99993665, 1.        , 0.99569211, 1.        ,\n",
    "       1.        , 0.99575546, 1.        , 1.        ]), 'mean_train_score': np.array([0.99649435, 1.        , 1.        , 0.99662105, 1.        ,\n",
    "       1.        , 0.99573405, 1.        , 1.        , 0.99630425,\n",
    "       1.        , 1.        , 0.99569183, 1.        , 1.        ,\n",
    "       0.99638876, 0.99997888, 1.        , 0.99592415, 1.        ,\n",
    "       1.        , 0.9964099 , 1.        , 1.        ]), 'std_train_score': np.array([6.25733424e-04, 0.00000000e+00, 0.00000000e+00, 7.91906591e-05,\n",
    "       0.00000000e+00, 0.00000000e+00, 4.46206266e-04, 0.00000000e+00,\n",
    "       0.00000000e+00, 4.46177186e-04, 0.00000000e+00, 0.00000000e+00,\n",
    "       2.68995301e-04, 0.00000000e+00, 0.00000000e+00, 2.36922985e-04,\n",
    "       2.98640811e-05, 0.00000000e+00, 2.44267970e-04, 0.00000000e+00,\n",
    "       0.00000000e+00, 7.53630262e-04, 0.00000000e+00, 0.00000000e+00])})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.00s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 0.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 1928.47s to fit \n"
     ]
    }
   ],
   "source": [
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X['NYCgov_Pov_Stat'].replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}})\n",
    "X = X.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data with sampling strategies (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "# 5) classifiers\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "#samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "scaler = Normalizer()\n",
    "classifier = BalancedRandomForestClassifier(n_estimators=1000, max_features=100, sampling_strategy='auto')\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                              ('dummies', dummy_encoder),\n",
    "                              ('scaler', scaler),\n",
    "                              ('clf', classifier)], \n",
    "                      memory=cachedir)\n",
    "                    \n",
    "t0 = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "time_to_fit = time.time() - t0\n",
    "print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0005020056104968013, 'JWTR_4'),\n",
       " (0.0006180397991121181, 'Boro_10'),\n",
       " (0.0006182323610456678, 'ENG_11'),\n",
       " (0.0006666647803905448, 'CIT_10'),\n",
       " (0.000845561954102215, 'Ethnicity_3'),\n",
       " (0.0010203369930211923, 'JWTR_3'),\n",
       " (0.0016273172555736843, 'INTP_adj_2'),\n",
       " (0.002282882484302701, 'Ethnicity_11'),\n",
       " (0.002428528916065317, 'AGEP_1'),\n",
       " (0.0039253585625141215, 'DIS_10'),\n",
       " (0.005603247830117554, 'JWTR_2'),\n",
       " (0.006984126312808381, 'INTP_adj_1'),\n",
       " (0.011814645668473192, 'Boro_1'),\n",
       " (0.016108428339244498, 'JWTR_1'),\n",
       " (0.05686880032439872, 'ENG_10')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline.named_steps['clf'].feature_importances_\n",
    "imps = list(zip(pipeline.named_steps['clf'].feature_importances_, X_train.columns))\n",
    "sorted(imps, key=lambda tup: tup[0])[-15:]\n",
    "#geometric_mean_score(y_test, pipeline.predict(X_test)) # 0.901335010891502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 0.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 0.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 1172.32s to fit \n"
     ]
    }
   ],
   "source": [
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X['NYCgov_Pov_Stat'].replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}})\n",
    "X = X.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data with sampling strategies (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "# 5) classifiers\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "#samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "scaler = Normalizer()\n",
    "classifier = RandomForestClassifier(n_estimators=1000, max_features=100)\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                              ('dummies', dummy_encoder),\n",
    "                              ('scaler', scaler),\n",
    "                              ('clf', classifier)], \n",
    "                      memory=cachedir)\n",
    "                    \n",
    "t0 = time.time()\n",
    "pipeline.fit(X_train, y_train)\n",
    "time_to_fit = time.time() - t0\n",
    "print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " <a list of 15 Text xticklabel objects>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imps = list(zip(X_train.columns, pipeline.named_steps['clf'].feature_importances_))\n",
    "imps=(sorted(imps, key=lambda tup: tup[1]))\n",
    "imps.reverse()\n",
    "#geometric_mean_score(y_test, pipeline.predict(X_test)) # 0.8741667300789631\n",
    "labels_i = [x[0] for x in imps][:15]\n",
    "ys_i = [x[1] for x in imps][:15]\n",
    "plt.plot(labels_i, ys_i)\n",
    "plt.xticks(rotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - No Financials\n",
    "This will be as above, but pulling out the financial variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AGEP_1</th>\n",
       "      <th>AGEP_10</th>\n",
       "      <th>AGEP_11</th>\n",
       "      <th>AGEP_12</th>\n",
       "      <th>AGEP_13</th>\n",
       "      <th>AGEP_14</th>\n",
       "      <th>AGEP_15</th>\n",
       "      <th>AGEP_16</th>\n",
       "      <th>AGEP_17</th>\n",
       "      <th>AGEP_18</th>\n",
       "      <th>...</th>\n",
       "      <th>WKW_19</th>\n",
       "      <th>WKW_2</th>\n",
       "      <th>WKW_20</th>\n",
       "      <th>WKW_3</th>\n",
       "      <th>WKW_4</th>\n",
       "      <th>WKW_5</th>\n",
       "      <th>WKW_6</th>\n",
       "      <th>WKW_7</th>\n",
       "      <th>WKW_8</th>\n",
       "      <th>WKW_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>Povunit_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1521345</th>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521371</th>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521389</th>\n",
       "      <th>1</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521399</th>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521415</th>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AGEP_1  AGEP_10  AGEP_11  AGEP_12  AGEP_13  AGEP_14  \\\n",
       "SERIALNO Povunit_ID                                                        \n",
       "1521345  1             32.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521371  1             32.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521389  1             57.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521399  1             39.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521415  1             36.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "                     AGEP_15  AGEP_16  AGEP_17  AGEP_18  ...    WKW_19  WKW_2  \\\n",
       "SERIALNO Povunit_ID                                      ...                    \n",
       "1521345  1               0.0      0.0      0.0      0.0  ...       0.0    0.0   \n",
       "1521371  1               0.0      0.0      0.0      0.0  ...       0.0    0.0   \n",
       "1521389  1               0.0      0.0      0.0      0.0  ...       0.0    1.0   \n",
       "1521399  1               0.0      0.0      0.0      0.0  ...       0.0    1.0   \n",
       "1521415  1               0.0      0.0      0.0      0.0  ...       0.0    3.0   \n",
       "\n",
       "                     WKW_20  WKW_3  WKW_4  WKW_5  WKW_6  WKW_7  WKW_8  WKW_9  \n",
       "SERIALNO Povunit_ID                                                           \n",
       "1521345  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521371  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521389  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521399  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521415  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = all_2016.copy()\n",
    "\n",
    "categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "# We'll create separate dataframes for personal and poverty-unit variables, then join them together\n",
    "personal_columns = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro']\n",
    "pu_columns = ['NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU', 'NYCgov_Pov_Stat']\n",
    "\n",
    "# Create a dataframe for the personal columns, including our 3 indicator variables\n",
    "X2_pers = X2.copy()\n",
    "X2_pers_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + personal_columns\n",
    "X2_pers = X2_pers[X2_pers_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "X2_pers = X2_pers.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Turn the multi-index columns into a single indexed column: 'AGEP_1', 'AGEP_2', 'AGEP_3', etc.\n",
    "X2_pers.columns = list(map('_'.join, [(y, str(z)) for y, z in (x for x in X2_pers.columns)]))\n",
    "\n",
    "# Create a dataframe for the poverty-unit columns, including our 3 indicator variables\n",
    "X2_pu = X2.copy()\n",
    "X2_pu_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + pu_columns\n",
    "X2_pu = X2_pu[X2_pu_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "X2_pu = X2_pu.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Groupby and take the max of SPORDER (these are poverty-unit variables; if there is a nonzero value, it's unique)\n",
    "X2_pu = X2_pu.stack().groupby(['SERIALNO', 'Povunit_ID']).max()\n",
    "\n",
    "# Add the personal and poverty-unit dataframes\n",
    "X2 = X2_pers.add(X2_pu, fill_value=0)\n",
    "X2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 1.03s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n",
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/imblearn/pipeline.py:197: UserWarning: Persisting input arguments took 0.64s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 977.44s to fit \n"
     ]
    }
   ],
   "source": [
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y2 = X2['NYCgov_Pov_Stat'].replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}})\n",
    "X2 = X2.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, stratify=y2)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data with sampling strategies (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "# 5) classifiers\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "#samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "scaler = Normalizer()\n",
    "classifier = RandomForestClassifier(n_estimators=1000, max_features=100)\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                              ('dummies', dummy_encoder),\n",
    "                              ('scaler', scaler),\n",
    "                              ('clf', classifier)], \n",
    "                      memory=cachedir)\n",
    "                    \n",
    "t0 = time.time()\n",
    "pipeline.fit(X2_train, y2_train)\n",
    "time_to_fit = time.time() - t0\n",
    "print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0019536750561223435, 'CIT_16'),\n",
       " (0.0019766238091623893, 'CIT_8'),\n",
       " (0.002074372847437007, 'CIT_19'),\n",
       " (0.002103638190090756, 'CIT_7'),\n",
       " (0.0021084495403344244, 'CIT_12'),\n",
       " (0.002192773240239379, 'CIT_20'),\n",
       " (0.0022114526799705417, 'CIT_5'),\n",
       " (0.0022167018769647475, 'CIT_15'),\n",
       " (0.0023101070754851017, 'CIT_18'),\n",
       " (0.0023378895980513356, 'DIS_7'),\n",
       " (0.0023659286886432707, 'CIT_6'),\n",
       " (0.0024051439784516827, 'CIT_3'),\n",
       " (0.0024677871327048293, 'CIT_17'),\n",
       " (0.002469216276971846, 'CIT_14'),\n",
       " (0.002591447212012863, 'CIT_4')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline.named_steps['clf'].feature_importances_\n",
    "imps = list(zip(pipeline.named_steps['clf'].feature_importances_, X2_train.columns))\n",
    "sorted(imps, key=lambda tup: tup[0])[-15:]\n",
    "#geometric_mean_score(y2_test, pipeline.predict(X2_test)) # 0.5891144868773415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'INTP_adj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-1fcd7c603bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_2016\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTP_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#len(all_2016[all_2016.INTP_adj > 0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTP_adj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'INTP_adj'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'INTP_adj'"
     ]
    }
   ],
   "source": [
    "all_2016.INTP_adj.describe()\n",
    "#len(all_2016[all_2016.INTP_adj > 0])\n",
    "X.loc[X.INTP_adj > 0, 'INTP_adj'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chachi/miniconda3/envs/pandas-tutorial/lib/python3.7/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    24259.000000\n",
       "mean      1623.764093\n",
       "std        994.621840\n",
       "min          4.030352\n",
       "25%        967.284480\n",
       "50%       1410.623200\n",
       "75%       2115.934800\n",
       "max       6448.563200\n",
       "Name: RNTP+MRGP, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = all_2016.copy()\n",
    "\n",
    "categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "# We'll create separate dataframes for personal and poverty-unit variables, then join them together\n",
    "#personal_columns = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj', 'Ethnicity', 'Boro']\n",
    "personal_nums = ['WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj'] \n",
    "#pu_columns = ['NP', 'TEN', 'HHT', 'MRGP_adj', 'RNTP_adj', 'HousingStatus', 'TotalWorkHrs_PU', 'NYCgov_Pov_Stat']\n",
    "\n",
    "# Create a dataframe for the personal columns, including our 3 indicator variables\n",
    "#X_pers = X.copy()\n",
    "#X_pers_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + personal_columns\n",
    "#X_pers = X_pers[X_pers_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "#X_pers = X_pers.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Turn the multi-index columns into a single indexed column: 'AGEP_1', 'AGEP_2', 'AGEP_3', etc.\n",
    "#X_pers.columns = list(map('_'.join, [(y, str(z)) for y, z in (x for x in X_pers.columns)]))\n",
    "\n",
    "# Create a dataframe for the poverty-unit columns, including our 3 indicator variables\n",
    "X_trials = trials.copy()\n",
    "X_trials_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + personal_nums\n",
    "X_trials = X_trials[X_trials_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "X_trials = X_trials.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Groupby and take the max of SPORDER (these are poverty-unit variables; if there is a nonzero value, it's unique)\n",
    "X_trials = X_trials.stack().groupby(['SERIALNO', 'Povunit_ID']).sum()\n",
    "\n",
    "# Add the personal and poverty-unit dataframes\n",
    "#X = X_pers.add(X_pu, fill_value=0)\n",
    "#X.tail()\n",
    "#X_trials[X_trials.OI_adj > 0].OI_adj.describe()\n",
    "#X_trials.MRGP_adj.describe()\n",
    "#all_2016[all_2016.Povunit_Rel == 1].RNTP_adj.describe()\n",
    "#all_2016.loc[(all_2016.Povunit_Rel == 1) & (all_2016.RNTP_adj > 0)].RNTP_adj.describe()\n",
    "all_2016['RNTP+MRGP'] = all_2016.RNTP_adj + all_2016.MRGP_adj\n",
    "all_2016.loc[(all_2016.Povunit_Rel == 1) & (all_2016['RNTP+MRGP'] > 0)]['RNTP+MRGP'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SERIALNO  SPORDER  Povunit_ID  ENG  WKW  TotalWorkHrs_PU\n",
      "710612        39        1           1  1.0  0.0                5\n",
      "710613        55        1           1  5.0  1.0                1\n",
      "710614        55        2           1  5.0  1.0                1\n",
      "710615        55        3           1  5.0  2.0                1\n",
      "710616        55        4           1  5.0  5.0                1\n",
      "710617        55        5           1  5.0  4.0                1\n",
      "710618        69        1           1  5.0  1.0                3\n",
      "710619       210        1           1  5.0  1.0                2\n",
      "710620       261        1           1  5.0  1.0                2\n",
      "710621       261        2           1  5.0  0.0                2\n",
      "        SERIALNO  SPORDER  Povunit_ID  ENG  WKW  TotalWorkHrs_PU\n",
      "710612        39        1           1    4    0                0\n",
      "710613        55        1           1    5    6                4\n",
      "710614        55        2           1    5    6                4\n",
      "710615        55        3           1    5    5                4\n",
      "710616        55        4           1    5    2                4\n",
      "710617        55        5           1    5    3                4\n",
      "710618        69        1           1    5    6                2\n",
      "710619       210        1           1    5    6                3\n",
      "710620       261        1           1    5    6                3\n",
      "710621       261        2           1    5    0                3\n"
     ]
    }
   ],
   "source": [
    "fX = all_2016.copy()\n",
    "print(fX[['SERIALNO', 'SPORDER', 'Povunit_ID', 'ENG', 'WKW', 'TotalWorkHrs_PU']].head(10))\n",
    "fix_orders = {'ENG': {0:0, 4:1, 3:2, 2:3, 1:4, 5:5}, 'WKW': {0:0, 6:1, 5:2, 4:3, 3:4, 2:5, 1:6}, \n",
    "              'TotalWorkHrs_PU': {5:0, 4:1, 3:2, 2:3, 1:4}}\n",
    "#fX[['SERIALNO', 'SPORDER', 'Povunit_ID', 'ENG', 'WKW', 'TotalWorkHrs_PU']].map(fix_orders).head(10)\n",
    "fX['ENG'] = fX['ENG'].map(fix_orders['ENG'])\n",
    "fX['WKW'] = fX['WKW'].map(fix_orders['WKW'])\n",
    "fX['TotalWorkHrs_PU'] = fX['TotalWorkHrs_PU'].map(fix_orders['TotalWorkHrs_PU'])\n",
    "print(fX[['SERIALNO', 'SPORDER', 'Povunit_ID', 'ENG', 'WKW', 'TotalWorkHrs_PU']].head(10))\n",
    "\n",
    "#Number of adults, number of kids, number of retirement-age adults, number of working-age adults, any kids, \n",
    "#any retirement-age adults\n",
    "fX['n_adults'] = fX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
