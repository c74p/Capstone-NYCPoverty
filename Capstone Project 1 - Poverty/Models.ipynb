{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### The Dataset\n",
    "\n",
    "As a refresher:\n",
    "* Data from https://data.cityofnewyork.us/browse?q=poverty\n",
    "* 12 annual data files, from 2005 to 2016 inclusive (e.g. NYCgov_Poverty_MeasureData__2016.csv)\n",
    "* CSV files with ~80 columns and ~60,000 rows each\n",
    "* Each file had essentially the same format and contained (mostly) the same information\n",
    "* Data types included:\n",
    "    * Classification types encoded as integers (e.g. 1 if in poverty, 2 if not in poverty)\n",
    "    * Floats for financial data (e.g. wages for the calendar year)\n",
    "\n",
    "I'll import a cleaned version of the files (see https://github.com/c74p/Springboard/blob/master/Capstone%20Project%201%20-%20Poverty/DataWranglingSummary.ipynb) for details.\n",
    "\n",
    "### Modeling approach\n",
    "\n",
    "The poverty rate overall in New York City is roughly 20%, and there are lots of imbalanced groups (education, income, \n",
    "disability status, etc.).  I'll use imbalanced test-train splits to improve my model.\n",
    "\n",
    "Overview of modeling approach:\n",
    "1. Use all years, households only, classify yes/no for poverty. Test and compare Logistic Regression, Support Vector \n",
    "Machines (SVM), and Random Forest algorithms.\n",
    "2. Run classifiers for individual years (the thresholds differ from year to year, so a predictor for a specific year would presumably be better for a specific year).\n",
    "3. Test running regressors on houshold income and poverty threshold, in order to predict poverty classification. Test and\n",
    "compare Linear Regression (Ordinary Least Squares), Stochastic Gradient Descent, and ElasticNet.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "4. Test steps 2 and 3 above at the person level, rather than at the household level.\n",
    "    a. This is not likely to be useful, but I'm doing it as a learning exercise.\n",
    "\n",
    "### Housekeeping part 1: imports and file prep\n",
    "\n",
    "After importing we'll make some quick modifications to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports and setup\n",
    "# See below for model-specific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Model-specific imports\n",
    "from dask_ml.preprocessing import Categorizer, DummyEncoder\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier, RUSBoostClassifier\n",
    "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, QuantileTransformer, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Logit, LogitResults\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Temporarily turn off warnings if they get to be too much\n",
    "#import warnings\n",
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_years.csv', index_col=0)\n",
    "\n",
    "# Group the columns into 1) raw input variables, 2) id variables of various things, 3) American Community Survey (census)\n",
    "# variables, 4) NYC government-calculated variables, and 5) output variables.\n",
    "#\n",
    "# The ACS and NYC variables are generally calculated from the raw input variables - my initial expectation is that\n",
    "# the raw input variables can be thought of as independent variables, and that the ACS and NYC variables are not\n",
    "# independent even though they are not output variables.\n",
    "\n",
    "raw_inp_vars = ['AGEP', 'Boro', 'CIT', 'DIS', 'ENG', 'ESR', 'Ethnicity', 'HHT', 'HIUnit_Head', 'HousingStatus', 'JWTR', 'LANX', 'MAR', 'MSP','NP', 'Off_Threshold', 'PreTaxIncome_PU', 'REL', 'SCH', 'SCHG', 'SCHL', 'SEX', 'TEN', 'WKHP', 'WKW', 'Year']\n",
    "id_vars = ['HIUnit_ID', 'Povunit_ID', 'PWGTP', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'WGTP']\n",
    "acs_vars = ['AgeCateg', 'INTP_adj', 'OI_adj', 'MRGP_adj', 'PA_adj', 'RETP_adj', 'RNTP_adj', 'SEMP_adj', 'SSIP_adj', 'SSP_adj',  'WAGP_adj']\n",
    "nyc_vars = ['CitizenStatus',  'EducAttain', 'FTPTWork', 'FamType_PU', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_EITC', 'NYCgov_FICAtax', 'NYCgov_HEAP', 'NYCgov_Housing', 'NYCgov_Income', 'NYCgov_IncomeTax', 'NYCgov_MOOP', 'NYCgov_MedPremiums', 'NYCgov_MedSpending', 'NYCgov_Nutrition', 'NYCgov_REL', 'NYCgov_SFN', 'NYCgov_SFR', 'NYCgov_SNAP', 'NYCgov_SchoolBreakfast', 'NYCgov_SchoolLunch', 'NYCgov_Threshold', 'NYCgov_WIC', 'Povunit_Rel', 'SNAPUnit_Rel',  'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'TaxUnit_Rel', 'TotalWorkHrs_PU']\n",
    "output_vars = ['NYCgov_PovGap', 'NYCgov_Pov_Stat', 'NYCgov_PovGapIndex', 'Off_Pov_Stat']\n",
    "all_columns = raw_inp_vars + id_vars + acs_vars + nyc_vars + output_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create codes for the raw input variables that are number-coded, so we can create charts that make sense\n",
    "raw_codes = {'Boro': {1: 'Bronx', 2: 'Brooklyn', 3: 'Manhattan', 4: 'Queens', 5: 'Staten Island'},\n",
    "         'CIT': {1: 'Birth', 2: 'Territories', 3: 'US Parents', 4: 'Naturalized', 5: 'No'},\n",
    "         'DIS': {0: 'NA', 1: 'Yes', 2: 'No'},\n",
    "         'ENG': {0: '<5', 1: 'Very Well', 2: 'Well', 3: 'Not well', 4: 'Not at all', 5: 'Only Eng'},\n",
    "         'ESR': {0: '<16', 1: 'EMP', 2:'EMP/NAW', 3: 'UNEMP', 4: 'AF', 5: 'AF/NAW', 6:'NILF'},\n",
    "         'Ethnicity': {1: 'White', 2: 'Black', 3: 'Asian', 4: 'Hispanic', 5: 'Other'},\n",
    "         'HHT': {0: 'NA', 1: 'MAR', 2: 'MNW', 3: 'WNM', 4: 'Malone', 5: 'MNAlone', 6: 'Walone', 7: 'WNalone'},\n",
    "         'HIUnit_Head': {0: 'Not Head', 1: 'Head', 2: 'Not Head'},\n",
    "         'HousingStatus': {0: 'NA', 1: 'Public', 2: 'Mitchell', 3: 'Subsidy', 4: 'Regulated', 5: 'OtherReg', 6: 'MarketRate', 7: 'NoCash', 8: 'OwnF&C', 9: 'Own-Mortgage'},\n",
    "         'JWTR': {0: 'NA', 1: 'Car', 2: 'Bus', 3:'Streetcar', 4:'Subway', 5:'RR', 6:'Ferry', 7:'Taxi', 8:'Motorcycle', 9:'Bike', 10:'Walk', 11:'Home', 12: 'Other'},\n",
    "         'LANX': {0: 'NA', 1: 'Yes', 2: 'Only Eng'},\n",
    "         'MAR': {1: 'Married', 2:'Widowed', 3:'Divorced', 4:'Separated', 5:'Never Married'},\n",
    "         'MSP': {0: 'NA', 1: 'Yes', 2:'Spouse absent', 3:'Widowed', 4:'Divorced', 5:'Separated', 6:'Never Married'},\n",
    "         'REL': {0: 'Self', 1:'Spouse', 2:'Child', 3:'Adopted', 4:'Stepchild', 5:'Sibling', 6:'Parent', 7:'Grandchild', 8:'Parent-in-law', 9:'Child-in-law', 10:'Other', 11:'Boarder', 12:'Roommate', 13:'Partner', 14:'Foster', 15:'OtherNR', 16:'Inst', 17:'NonInst'},\n",
    "         'SCH': {0: 'NA', 1: 'NoPast3Mos', 2:'Public', 3:'Private/Home'},\n",
    "         'SCHG': {0: 'NA', 1:'Preschool', 2:'Kindergarten', 3:'1', 4:'2', 5:'3', 6:'4', 7:'5', 8:'6', 9:'7', 10:'8', 11:'9', 12:'10', 13:'11', 14:'12', 15:'College', 16:'Grad school'},\n",
    "         'SCHL': {0: 'NA', 1:'None', 2:'Preschool', 3:'Kindergarten', 4:'1', 5:'2', 6:'3', 7:'4', 8:'5', 9:'6', 10:'7', 11:'8', 12:'9', 13:'10', 14:'11', 15:'12-NoDip', 16:'Diploma', 17:'GED', 18:'<1yrCollege', 19:'CollNoDegree', 20:'Associates', 21:'Bachelors', 22:'Masters', 23:'Professional', 24:'Doctorate'},\n",
    "         'SEX': {1:'Male', 2:'Female'},\n",
    "         'TEN': {0: 'NA', 1:'Mortage', 2:'Free&Clear', 3:'Rent', 4:'OccButNoRent'},\n",
    "         'WKW': {0:'NA', 1:'50-52', 2:'48-49', 3:'40-47', 4:'27-39', 5:'14-26', 6:'<13'},\n",
    "        }\n",
    "\n",
    "# Create codes for the nyc variables that are number-coded, so we can create charts that make sense\n",
    "nyc_codes = {\n",
    "    'CitizenStatus': {1: 'Birth', 2: 'Naturalized', 3: 'No'},\n",
    "    'EducAttain': {0: 'NA', 1:'<HS', 2:'HS', 3:'SomeCollege', 4:'Bachelors+'},\n",
    "    'FTPTWork': {1:'FTYR', 2:'<FTYR', 3:'None'},\n",
    "    'FamType_PU': {1:'Family', 2:'Couple', 3:'M+kid', 4:'W+kid', 5:'Mnokid', 6:'Wnokid', 7:'Unrelated', 8:'UnrelAlone'},\n",
    "    'NYCgov_REL': {0:'Self', 1:'Spouse', 2:'Child', 3:'Sibling', 4:'Parent', 5:'Grandkid', 6:'Inlaw', 7:'OtherRel', 8:'Boarder', 9:'Roommate', 10:'Partner', 11:'FosterKid', 12:'OtherNonRel'},\n",
    "    'NYCgov_SFR': {0: 'NA', 1:'NoKids', 2:'Kids', 3:'OneParent', 4:'Kid', 5:'Kid-Monly', 6:'Kid-Wonly'},\n",
    "    'Povunit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'SNAPUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other'},\n",
    "    'TaxUnit_FILER': {1:'Filer', 0:'Non-Filer'},\n",
    "    'TaxUnit_FILESTAT': {0: 'NA', 1:'Joint', 2:'HH', 3:'MFS', 4:'Single'},\n",
    "    'TaxUnit_FILETYPE': {0: 'NA', 1: 'Normal', 2:'Dependent', 3:'BelowThresh'},\n",
    "    'TaxUnit_Rel': {1:'Head', 2:'Spouse/Ptnr', 3:'Child', 4:'Other', 5:'EIC', 6:'Relative'},\n",
    "    'TotalWorkHrs_PU': {1:'3500+', 2:'2340-3500', 3:'1750-2340', 4:'<1750', 5:'None'}\n",
    "    }\n",
    "\n",
    "# Create a dataframe 'cats' that uses categorical coding, rather than numerical coding, based on the dictionaries above.\n",
    "#cats = df.replace(nyc_codes)\n",
    "#cats = cats.replace(raw_codes)\n",
    "#cats = cats.replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}, \n",
    "                     #'Off_Pov_Stat': {1:'Pov', 2:'Not Pov'}, \n",
    "                     #'AgeCateg': {1: 'U18', 2:'18-64', 3:'65+'}})\n",
    "\n",
    "# Update one column so that NA's are all in one category\n",
    "#cats.loc[cats['HIUnit_Head'].isna(), 'HIUnit_Head'] = 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key dataframes of interest\n",
    "\n",
    "# cats is already listed above - dataframe with all of the category variables un-encoded\n",
    "\n",
    "# All 2016 data\n",
    "all_2016 = df[df.Year == 2016]\n",
    "#cats_2016 = cats[cats.Year == 2016]\n",
    "\n",
    "# 2016 data for poverty units only\n",
    "# For example, the data dictionary, sheet \"Column Info\", cell D81, says in relation to calculating the poverty gap:\n",
    "# \"retain only the reference person of each family in poverty (Povunit_Rel==1 & NYCgov_Poverty == 1)\"\n",
    "pu_2016 = df[(df.Year == 2016) & (df.Povunit_Rel == 1)]\n",
    "#pu_cats_2016 = cats[(cats.Year == 2016) & (cats.Povunit_Rel == 'Head')]\n",
    "#pu_cats_all_years = cats[(cats.Povunit_Rel == 'Head')]\n",
    "\n",
    "# Our data set contains two sets of weights: household weights and person weights.  \n",
    "# We need to separate out each column by whether it should be weighted as a household variable or a person variable.\n",
    "# Lists to create weighted columns, separated based on whether they are personal or household statistics.\n",
    "personal_vars = ['AGEP', 'Boro', 'CIT', 'SCH', 'SCHG', 'SCHL', 'SEX', 'ESR', 'ENG', 'LANX', 'MSP', 'MAR', 'NYCgov_EITC', 'WKW', 'WKHP', 'DIS', 'JWTR', 'WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj', 'TaxUnit_Rel', 'NYCgov_REL', 'NYCgov_SFR', 'SNAPUnit_Rel', 'TaxUnit_FILER', 'TaxUnit_FILESTAT', 'TaxUnit_FILETYPE', 'Ethnicity', 'EducAttain', 'CitizenStatus', 'AgeCateg', 'FTPTWork', 'PWGTP'] \n",
    "pu_vars = ['MRGP_adj', 'RNTP_adj', 'NP', 'TEN', 'HHT', 'FamType_PU', 'HousingStatus', 'TotalWorkHrs_PU', 'PreTaxIncome_PU', 'NYCgov_Income', 'NYCgov_Threshold', 'NYCgov_Pov_Stat',  'NYCgov_Housing', 'NYCgov_Childcare', 'NYCgov_Commuting', 'NYCgov_MOOP', 'NYCgov_MedSpending', 'NYCgov_MedPremiums', 'NYCgov_HEAP', 'NYCgov_WIC', 'NYCgov_SNAP', 'NYCgov_SchoolLunch', 'NYCgov_SchoolBreakfast', 'NYCgov_Nutrition', 'NYCgov_FICAtax', 'NYCgov_IncomeTax', 'Off_Threshold', 'Off_Pov_Stat', 'NYCgov_PovGap', 'NYCgov_PovGapIndex', 'WGTP']\n",
    "other_vars = ['HIUnit_Head', 'HIUnit_ID', 'NYCgov_SFN', 'Povunit_ID', 'Povunit_Rel', 'REL', 'SERIALNO', 'SNAPUnit_ID', 'SPORDER', 'TaxUnit_ID', 'Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Categorical Variables\n",
    "Creating our categorical variables happens logically *after* our feature engineering, but it builds on some of the\n",
    "structure in our data set *before* we have the new features.  So, let's do it now; if this doesn't make sense, look at \n",
    "the next Feature Engineering section and it should make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1]\n",
       "Categories (5, int64): [5 < 4 < 3 < 2 < 1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = all_2016.copy()\n",
    "\n",
    "# All the categoricals that we'll have to set up\n",
    "categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "categories = {} # Dict for each initial categorical\n",
    "\n",
    "# Some categoricals have odd ordering; some have no ordering\n",
    "odd_ordering = {'ENG': [0, 4, 3, 2, 1, 5], 'WKW': [0, 6, 5, 4, 3, 2, 1], 'TotalWorkHrs_PU': [5, 4, 3, 2, 1]}\n",
    "unordered = ['DIS', 'SEX', 'MSP', 'JWTR', 'Ethnicity', 'Boro', 'TEN', 'HHT', 'HousingStatus']\n",
    "\n",
    "# Loop through and assign appropriate category structure\n",
    "for feature in categoricals:\n",
    "    if feature in odd_ordering.keys():\n",
    "        cats = odd_ordering[feature]\n",
    "    else:\n",
    "        cats = X[feature].unique()\n",
    "        cats.sort()\n",
    "    if feature in unordered:\n",
    "        categories[feature] = pd.Categorical(cats, ordered=False)\n",
    "    else: # Category is ordered\n",
    "        categories[feature] = pd.Categorical(cats, ordered=True, categories=cats)\n",
    "\n",
    "# Create a dictionary 'dummy_these' that we'll pass to our dummy-maker later\n",
    "# The poverty-unit categoricals can be passed as-is\n",
    "# For the personal categoricals, we'll have features like 'AGEP_1', 'AGEP_2', ..., 'AGEP_20' - so we have to \n",
    "# loop through and assign categories\n",
    "personal_categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro']\n",
    "pu_categoricals = ['NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "dummy_these = {}\n",
    "    \n",
    "for i in range(1,21):\n",
    "    for feature in personal_categoricals:\n",
    "        name = feature + '_' + str(i)\n",
    "        dummy_these[name] = categories[feature]\n",
    "\n",
    "for feature in pu_categoricals:\n",
    "    dummy_these[feature] = categories[feature]\n",
    "\n",
    "len(dummy_these) #245\n",
    "len(personal_categoricals) #12\n",
    "len(pu_categoricals)\n",
    "#len(categoricals)\n",
    "#len(personal_columns)\n",
    "#print(categoricals)\n",
    "#print(personal_columns)\n",
    "dummy_these['TotalWorkHrs_PU']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Features\n",
    "Our dataset contains people in poverty units (a household may contain one or more poverty units). The entire \n",
    "poverty unit either is or is not in poverty, but the data set as constructed has people in different rows (the data is\n",
    "not tidy).\n",
    "\n",
    "To tidy up, we'll move information on all the people in the poverty unit, into the row for that poverty unit.\n",
    "Instead of having 3 people in a poverty unit represented by different rows, we'll put all three people in\n",
    "the same row but different columns. The columns will be named 'AGEP_1', 'AGEP_2', 'AGEP_3', etc, with zero values in all\n",
    "columns where person n does not exist.\n",
    "\n",
    "There are three main columns of interest for this:\n",
    "* SERIALNO is the serial number of each household.\n",
    "* PovUnit_ID is the serial number of the poverty unit within the household (1-18). Each household can have more than one poverty unit (although the vast majority of households have only one poverty unit).\n",
    "* SPORDER is the serial number of a person in the household (1-20). Note that the dataset only assigns serial numbers to\n",
    "the people in the *household*, not the *poverty unit*.  This means that if for example a household has two poverty units,\n",
    "the first with two people and the second with three people, the head of the second poverty unit will have SPORDER of 3,\n",
    "not 1.  (One-based counting scheme) This is not a problem, but a particularity to be aware of when looking at dataset\n",
    "rows for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>AGEP_1</th>\n",
       "      <th>AGEP_10</th>\n",
       "      <th>AGEP_11</th>\n",
       "      <th>AGEP_12</th>\n",
       "      <th>AGEP_13</th>\n",
       "      <th>AGEP_14</th>\n",
       "      <th>AGEP_15</th>\n",
       "      <th>AGEP_16</th>\n",
       "      <th>AGEP_17</th>\n",
       "      <th>AGEP_18</th>\n",
       "      <th>...</th>\n",
       "      <th>WKW_19</th>\n",
       "      <th>WKW_2</th>\n",
       "      <th>WKW_20</th>\n",
       "      <th>WKW_3</th>\n",
       "      <th>WKW_4</th>\n",
       "      <th>WKW_5</th>\n",
       "      <th>WKW_6</th>\n",
       "      <th>WKW_7</th>\n",
       "      <th>WKW_8</th>\n",
       "      <th>WKW_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>Povunit_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1521345</th>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521371</th>\n",
       "      <th>1</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521389</th>\n",
       "      <th>1</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521399</th>\n",
       "      <th>1</th>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521415</th>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AGEP_1  AGEP_10  AGEP_11  AGEP_12  AGEP_13  AGEP_14  \\\n",
       "SERIALNO Povunit_ID                                                        \n",
       "1521345  1             32.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521371  1             32.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521389  1             57.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521399  1             39.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1521415  1             36.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "                     AGEP_15  AGEP_16  AGEP_17  AGEP_18  ...    WKW_19  WKW_2  \\\n",
       "SERIALNO Povunit_ID                                      ...                    \n",
       "1521345  1               0.0      0.0      0.0      0.0  ...       0.0    0.0   \n",
       "1521371  1               0.0      0.0      0.0      0.0  ...       0.0    0.0   \n",
       "1521389  1               0.0      0.0      0.0      0.0  ...       0.0    1.0   \n",
       "1521399  1               0.0      0.0      0.0      0.0  ...       0.0    1.0   \n",
       "1521415  1               0.0      0.0      0.0      0.0  ...       0.0    3.0   \n",
       "\n",
       "                     WKW_20  WKW_3  WKW_4  WKW_5  WKW_6  WKW_7  WKW_8  WKW_9  \n",
       "SERIALNO Povunit_ID                                                           \n",
       "1521345  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521371  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521389  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521399  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1521415  1              0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 408 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = all_2016.copy()\n",
    "\n",
    "categoricals = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'Ethnicity', 'Boro', 'NP', 'TEN', 'HHT', 'HousingStatus', 'TotalWorkHrs_PU']\n",
    "\n",
    "# We'll create separate dataframes for personal and poverty-unit variables, then join them together\n",
    "personal_columns = ['AGEP', 'CIT', 'SCHL', 'SEX', 'ENG', 'MSP', 'WKW', 'WKHP', 'DIS', 'JWTR', 'WAGP_adj', 'INTP_adj', 'SEMP_adj', 'SSP_adj', 'SSIP_adj', 'PA_adj', 'RETP_adj', 'OI_adj', 'Ethnicity', 'Boro']\n",
    "pu_columns = ['NP', 'TEN', 'HHT', 'MRGP_adj', 'RNTP_adj', 'HousingStatus', 'TotalWorkHrs_PU', 'NYCgov_Pov_Stat']\n",
    "\n",
    "# Create a dataframe for the personal columns, including our 3 indicator variables\n",
    "X_pers = X.copy()\n",
    "X_pers_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + personal_columns\n",
    "X_pers = X_pers[X_pers_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "X_pers = X_pers.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Turn the multi-index columns into a single indexed column: 'AGEP_1', 'AGEP_2', 'AGEP_3', etc.\n",
    "X_pers.columns = list(map('_'.join, [(y, str(z)) for y, z in (x for x in X_pers.columns)]))\n",
    "\n",
    "# Create a dataframe for the poverty-unit columns, including our 3 indicator variables\n",
    "X_pu = X.copy()\n",
    "X_pu_columns = ['SERIALNO', 'Povunit_ID', 'SPORDER'] + pu_columns\n",
    "X_pu = X_pu[X_pu_columns]\n",
    "\n",
    "# Grouping by SERIALNO and Povunit_ID, put SPORDER (person # in household) at the top as multi-index columns\n",
    "X_pu = X_pu.set_index(['SERIALNO', 'Povunit_ID', 'SPORDER']).unstack('SPORDER').fillna(0)\n",
    "\n",
    "# Groupby and take the max of SPORDER (these are poverty-unit variables; if there is a nonzero value, it's unique)\n",
    "X_pu = X_pu.stack().groupby(['SERIALNO', 'Povunit_ID']).max()\n",
    "\n",
    "# Add the personal and poverty-unit dataframes\n",
    "X = X_pers.add(X_pu, fill_value=0)\n",
    "X.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's hit it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# Pull off 'NYCgov_Pov_Stat' for our target variable\n",
    "y = X['NYCgov_Pov_Stat'].replace({'NYCgov_Pov_Stat': {1: 'Pov', 2:'Not Pov'}})\n",
    "X = X.drop('NYCgov_Pov_Stat', axis='columns')\n",
    "\n",
    "# Get train and test - be sure to stratify since this is imbalanced data (poverty ~20% of the set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Transforms for pipeline: \n",
    "# 1) categorize to prep for one-hot encoding\n",
    "# 2) one-hot encode, dropping one to avoid colinearity\n",
    "# 3) deal with imbalanced data with sampling strategies (poverty is ~20% of total)\n",
    "# 4) scale data\n",
    "# 5) classifiers\n",
    "categorizer = Categorizer(columns=dummy_these)\n",
    "dummy_encoder = DummyEncoder(drop_first=True)\n",
    "samplers = [SMOTE(random_state=42), SMOTETomek(random_state=42), TomekLinks(random_state=42)]\n",
    "#scalers = [StandardScaler(), Normalizer(), PowerTransformer(), QuantileTransformer()]\n",
    "scalers = [Normalizer()]\n",
    "scaler = Normalizer()\n",
    "#classifiers = [LogisticRegression(), SGDClassifier(), AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), \n",
    "               #RandomForestClassifier(), BalancedBaggingClassifier(), BalancedRandomForestClassifier(), RUSBoostClassifier()]\n",
    "#classifiers = [BalancedBaggingClassifier(), BaggingClassifier(), RandomForestClassifier(), BalancedRandomForestClassifier(), \n",
    "               #AdaBoostClassifier(), GradientBoostingClassifier()]\n",
    "#classifiers = [RandomForestClassifier(), BalancedRandomForestClassifier()]\n",
    "classifiers = [BalancedRandomForestClassifier()]\n",
    "\n",
    "#sampler = TomekLinks(random_state=42)\n",
    "#scaler = QuantileTransformer()\n",
    "#clf = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "#clf = RandomForestClassifier(n_estimators=100)\n",
    "#clf = AdaBoostClassifier()\n",
    "#params={0: {'clf__C': [1, 1e-1, 1e-2, 1e-3], 'clf__max_iter': [1e2, 1e3, 1e4], # Logistic Regression\n",
    "                               #'clf__solver': ['lbfgs', 'liblinear', 'sag', 'saga']}, \n",
    "        #1: {'n_estimators': [1e1, 1e2, 1e3], 'max_features': [5, 10, 50, 100], # Random Forest Classifier\n",
    "                         #'criterion': ['gini', 'entropy']}\n",
    "       #}\n",
    "#params = {0: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy']},\n",
    "          #1: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy'], 'clf_sampling_strategy': [0.05, 0.25, 0.5, 0.75, 0.95]}\n",
    "         #}\n",
    "\n",
    "params = {0: {'clf__n_estimators': [1000], 'clf__max_features': [100],\n",
    "              'clf__sampling_strategy': ['not minority', 'not majority', 'all']}}\n",
    "          #1: {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100],\n",
    "              #'clf__criterion': ['gini', 'entropy']},\n",
    "         #}\n",
    "\n",
    "#parameters = {'clf__n_estimators': [10, 100, 1000], 'clf__max_features': [5, 10, 50, 100], 'clf__criterion': ['gini', 'entropy']}\n",
    "parameters = {'clf__n_estimators': [100], 'clf__max_features': ['auto'], 'clf__criterion': ['gini']}\n",
    "\n",
    "cachedir = tempfile.mkdtemp()\n",
    "\n",
    "#pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                              #('dummies', dummy_encoder),\n",
    "                              #('sampler', sampler),\n",
    "                              #('scaler', scaler),\n",
    "                              #('clf', BalancedRandomForestClassifier())], \n",
    "                      #memory=cachedir)\n",
    "                    \n",
    "#grid = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=5, n_jobs=-1, pre_dispatch=2, verbose=9)#, scoring=balanced_accuracy_score())\n",
    "#grid = GridSearchCV(estimator=pipeline, param_grid=parameters, cv=5, n_jobs=-1, verbose=9)\n",
    "\n",
    "#t0 = time.time()\n",
    "#grid.fit(X_train, y_train)\n",
    "#time_to_fit = time.time() - t0\n",
    "#print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "#print(grid.cv_results_)\n",
    "\n",
    "#for sampler, i in zip(samplers, range(len(samplers))):\n",
    "for i in range(1):\n",
    "    #for scaler, j in zip(scalers, range(len(scalers))):\n",
    "    for scaler in scalers:\n",
    "        for k in range(len(classifiers)):\n",
    "            #pipeline = Pipeline(steps=[#('cat', categorizer),\n",
    "            pipeline = imbPipeline(steps=[('cat', categorizer),\n",
    "                                          ('dummies', dummy_encoder),\n",
    "                                          #('sampler', sampler),\n",
    "                                          ('scaler', scaler),\n",
    "                                          ('clf', classifiers[k])],\n",
    "                                  memory=cachedir)\n",
    "\n",
    "            #print(pipeline)\n",
    "            #print(params[i])\n",
    "            #pipeline.get_params().keys()\n",
    "            grid = GridSearchCV(estimator=pipeline, param_grid=params[k], cv=3, n_jobs=-1, verbose=9)#, scoring=balanced_accuracy_score())\n",
    "\n",
    "            t0 = time.time()\n",
    "            #pipeline.fit(X_train, y_train)\n",
    "            grid.fit(X_train, y_train)\n",
    "            time_to_fit = time.time() - t0\n",
    "            print('Took: ' + '{:4.2f}'.format(time_to_fit) + 's to fit ')\n",
    "            print(grid.cv_results_)\n",
    "            print('best estimator: ' + str(grid.best_estimator_))\n",
    "            print('best params: ' + str(grid.best_params_))\n",
    "            print('best index: ' + str(grid.best_index_))\n",
    "            \n",
    "            #print(str(sampler) + ',' + str(scaler) + ',' + str(classifiers[k]))\n",
    "            #print(str(scaler) + ',' + str(classifiers[k]))\n",
    "                  \n",
    "            #means = grid.cv_results_['mean_test_score']\n",
    "            #stds = grid.cv_results_['std_test_score']\n",
    "            #for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "                #print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "            #predictions = pipeline.predict(X_test)\n",
    "\n",
    "#print('Predictions: ' + str(predictions))\n",
    "#print('Actual:\\n' + str(y_small))\n",
    "            #print('\\nBalanced accuracy: ' + str(balanced_accuracy_score(y_test, predictions)))\n",
    "            #print('Geometric mean: ' + str(geometric_mean_score(y_test, predictions)))\n",
    "            #print('Confusion matrix:\\n' + str(confusion_matrix(y_test, predictions)))\n",
    "            #print('\\nClassification report:\\n' + str(classification_report_imbalanced(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7975358685378098"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometric_mean_score(grid.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.019535791275811728, 'WKW_2'),\n",
       " (0.020252231346152935, 'INTP_adj_1'),\n",
       " (0.020452137721584687, 'SCHL_2'),\n",
       " (0.020767468686063252, 'RNTP_adj'),\n",
       " (0.02515045254416771, 'WKHP_2'),\n",
       " (0.026205088630138082, 'AGEP_1'),\n",
       " (0.02817688155652038, 'SCHL_1'),\n",
       " (0.03238306328525264, 'WKW_1'),\n",
       " (0.03242418603775719, 'JWTR_1'),\n",
       " (0.03587014309356664, 'RETP_adj_1'),\n",
       " (0.041251391667779504, 'WKHP_1'),\n",
       " (0.05057339662820781, 'SSP_adj_1'),\n",
       " (0.052182437399624144, 'WAGP_adj_2'),\n",
       " (0.07156076688876317, 'TotalWorkHrs_PU'),\n",
       " (0.14873418922209963, 'WAGP_adj_1')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tester = BalancedRandomForestClassifier().fit(X=X_train, y=y_train)\n",
    "#len(tester.feature_importances_)\n",
    "#X_train.columns\n",
    "#geometric_mean_score(tester.predict(X_test), y_test)  #0.775727880752169\n",
    "imps = list(zip(tester.feature_importances_, X_train.columns))\n",
    "#imps.sort(key=lambda x: x[1])\n",
    "sorted(imps, key=lambda tup: tup[0])[-15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__criterion</th>\n",
       "      <th>param_clf__max_features</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.356531</td>\n",
       "      <td>0.774929</td>\n",
       "      <td>7.338355</td>\n",
       "      <td>0.130668</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.912454</td>\n",
       "      <td>0.913203</td>\n",
       "      <td>0.911165</td>\n",
       "      <td>0.912274</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>22</td>\n",
       "      <td>0.997085</td>\n",
       "      <td>0.995628</td>\n",
       "      <td>0.996769</td>\n",
       "      <td>0.996494</td>\n",
       "      <td>0.000626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.269967</td>\n",
       "      <td>2.907710</td>\n",
       "      <td>8.925081</td>\n",
       "      <td>2.030989</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.918662</td>\n",
       "      <td>0.920046</td>\n",
       "      <td>0.921176</td>\n",
       "      <td>0.919961</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.631243</td>\n",
       "      <td>11.630676</td>\n",
       "      <td>24.091638</td>\n",
       "      <td>2.337423</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.919422</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>0.919909</td>\n",
       "      <td>0.919919</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.915396</td>\n",
       "      <td>0.285015</td>\n",
       "      <td>8.093561</td>\n",
       "      <td>0.456263</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.911809</td>\n",
       "      <td>0.910404</td>\n",
       "      <td>0.910416</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>24</td>\n",
       "      <td>0.996515</td>\n",
       "      <td>0.996642</td>\n",
       "      <td>0.996706</td>\n",
       "      <td>0.996621</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.711045</td>\n",
       "      <td>0.981712</td>\n",
       "      <td>7.328075</td>\n",
       "      <td>1.507757</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.916255</td>\n",
       "      <td>0.920806</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.919116</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>151.528578</td>\n",
       "      <td>11.447976</td>\n",
       "      <td>19.518697</td>\n",
       "      <td>3.485998</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.919802</td>\n",
       "      <td>0.919792</td>\n",
       "      <td>0.921810</td>\n",
       "      <td>0.920468</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.778635</td>\n",
       "      <td>0.298928</td>\n",
       "      <td>7.595569</td>\n",
       "      <td>0.218479</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.917511</td>\n",
       "      <td>0.918388</td>\n",
       "      <td>0.917300</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>18</td>\n",
       "      <td>0.995248</td>\n",
       "      <td>0.995628</td>\n",
       "      <td>0.996326</td>\n",
       "      <td>0.995734</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36.483098</td>\n",
       "      <td>3.360073</td>\n",
       "      <td>6.941224</td>\n",
       "      <td>1.106543</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.922463</td>\n",
       "      <td>0.924354</td>\n",
       "      <td>0.925231</td>\n",
       "      <td>0.924016</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>353.551459</td>\n",
       "      <td>24.645494</td>\n",
       "      <td>14.723334</td>\n",
       "      <td>2.187714</td>\n",
       "      <td>gini</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.922970</td>\n",
       "      <td>0.925494</td>\n",
       "      <td>0.927005</td>\n",
       "      <td>0.925156</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.911089</td>\n",
       "      <td>0.593627</td>\n",
       "      <td>7.788339</td>\n",
       "      <td>0.047878</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.914608</td>\n",
       "      <td>0.914090</td>\n",
       "      <td>0.917754</td>\n",
       "      <td>0.915484</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>19</td>\n",
       "      <td>0.995818</td>\n",
       "      <td>0.996199</td>\n",
       "      <td>0.996896</td>\n",
       "      <td>0.996304</td>\n",
       "      <td>0.000446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62.250116</td>\n",
       "      <td>6.208748</td>\n",
       "      <td>7.382807</td>\n",
       "      <td>1.209777</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.927024</td>\n",
       "      <td>0.929676</td>\n",
       "      <td>0.925992</td>\n",
       "      <td>0.927564</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>604.099197</td>\n",
       "      <td>40.388460</td>\n",
       "      <td>15.118818</td>\n",
       "      <td>2.104008</td>\n",
       "      <td>gini</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'gini', 'clf__max_features'...</td>\n",
       "      <td>0.927277</td>\n",
       "      <td>0.927902</td>\n",
       "      <td>0.927893</td>\n",
       "      <td>0.927690</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.635937</td>\n",
       "      <td>0.202809</td>\n",
       "      <td>8.041993</td>\n",
       "      <td>0.094281</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.911187</td>\n",
       "      <td>0.911049</td>\n",
       "      <td>0.910784</td>\n",
       "      <td>0.911007</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>23</td>\n",
       "      <td>0.995311</td>\n",
       "      <td>0.995882</td>\n",
       "      <td>0.995882</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18.703423</td>\n",
       "      <td>0.274799</td>\n",
       "      <td>9.826621</td>\n",
       "      <td>0.189647</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.920182</td>\n",
       "      <td>0.919539</td>\n",
       "      <td>0.921049</td>\n",
       "      <td>0.920257</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>125.815334</td>\n",
       "      <td>10.479840</td>\n",
       "      <td>25.741467</td>\n",
       "      <td>3.183750</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.919676</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>0.920289</td>\n",
       "      <td>0.920130</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.241193</td>\n",
       "      <td>0.842785</td>\n",
       "      <td>6.291520</td>\n",
       "      <td>1.195959</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.913088</td>\n",
       "      <td>0.909275</td>\n",
       "      <td>0.915347</td>\n",
       "      <td>0.912570</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>21</td>\n",
       "      <td>0.996452</td>\n",
       "      <td>0.996642</td>\n",
       "      <td>0.996072</td>\n",
       "      <td>0.996389</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.362928</td>\n",
       "      <td>1.091575</td>\n",
       "      <td>8.315560</td>\n",
       "      <td>0.948133</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.920182</td>\n",
       "      <td>0.921946</td>\n",
       "      <td>0.922443</td>\n",
       "      <td>0.921524</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>9</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>141.452599</td>\n",
       "      <td>12.096369</td>\n",
       "      <td>18.503021</td>\n",
       "      <td>1.951633</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.921196</td>\n",
       "      <td>0.921820</td>\n",
       "      <td>0.920542</td>\n",
       "      <td>0.921186</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.892606</td>\n",
       "      <td>0.185923</td>\n",
       "      <td>7.927964</td>\n",
       "      <td>0.195101</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.914354</td>\n",
       "      <td>0.915864</td>\n",
       "      <td>0.914713</td>\n",
       "      <td>0.914977</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>20</td>\n",
       "      <td>0.996262</td>\n",
       "      <td>0.995819</td>\n",
       "      <td>0.995692</td>\n",
       "      <td>0.995924</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28.135110</td>\n",
       "      <td>1.850436</td>\n",
       "      <td>7.699444</td>\n",
       "      <td>1.007361</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.925250</td>\n",
       "      <td>0.926635</td>\n",
       "      <td>0.924978</td>\n",
       "      <td>0.925621</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>273.912406</td>\n",
       "      <td>17.282531</td>\n",
       "      <td>13.551803</td>\n",
       "      <td>1.713333</td>\n",
       "      <td>entropy</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.927151</td>\n",
       "      <td>0.927015</td>\n",
       "      <td>0.927386</td>\n",
       "      <td>0.927184</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.285676</td>\n",
       "      <td>0.373293</td>\n",
       "      <td>7.967199</td>\n",
       "      <td>0.167094</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.916002</td>\n",
       "      <td>0.920172</td>\n",
       "      <td>0.919402</td>\n",
       "      <td>0.918525</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>17</td>\n",
       "      <td>0.997466</td>\n",
       "      <td>0.996009</td>\n",
       "      <td>0.995755</td>\n",
       "      <td>0.996410</td>\n",
       "      <td>0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.109372</td>\n",
       "      <td>4.389925</td>\n",
       "      <td>7.425203</td>\n",
       "      <td>1.210115</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.926390</td>\n",
       "      <td>0.929042</td>\n",
       "      <td>0.926879</td>\n",
       "      <td>0.927437</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>356.878639</td>\n",
       "      <td>99.027548</td>\n",
       "      <td>11.762577</td>\n",
       "      <td>2.313773</td>\n",
       "      <td>entropy</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'clf__criterion': 'entropy', 'clf__max_featur...</td>\n",
       "      <td>0.928544</td>\n",
       "      <td>0.931323</td>\n",
       "      <td>0.927132</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       29.356531      0.774929         7.338355        0.130668   \n",
       "1       21.269967      2.907710         8.925081        2.030989   \n",
       "2      133.631243     11.630676        24.091638        2.337423   \n",
       "3        6.915396      0.285015         8.093561        0.456263   \n",
       "4       17.711045      0.981712         7.328075        1.507757   \n",
       "5      151.528578     11.447976        19.518697        3.485998   \n",
       "6        8.778635      0.298928         7.595569        0.218479   \n",
       "7       36.483098      3.360073         6.941224        1.106543   \n",
       "8      353.551459     24.645494        14.723334        2.187714   \n",
       "9       10.911089      0.593627         7.788339        0.047878   \n",
       "10      62.250116      6.208748         7.382807        1.209777   \n",
       "11     604.099197     40.388460        15.118818        2.104008   \n",
       "12       6.635937      0.202809         8.041993        0.094281   \n",
       "13      18.703423      0.274799         9.826621        0.189647   \n",
       "14     125.815334     10.479840        25.741467        3.183750   \n",
       "15       5.241193      0.842785         6.291520        1.195959   \n",
       "16      16.362928      1.091575         8.315560        0.948133   \n",
       "17     141.452599     12.096369        18.503021        1.951633   \n",
       "18       7.892606      0.185923         7.927964        0.195101   \n",
       "19      28.135110      1.850436         7.699444        1.007361   \n",
       "20     273.912406     17.282531        13.551803        1.713333   \n",
       "21       9.285676      0.373293         7.967199        0.167094   \n",
       "22      44.109372      4.389925         7.425203        1.210115   \n",
       "23     356.878639     99.027548        11.762577        2.313773   \n",
       "\n",
       "   param_clf__criterion param_clf__max_features param_clf__n_estimators  \\\n",
       "0                  gini                       5                      10   \n",
       "1                  gini                       5                     100   \n",
       "2                  gini                       5                    1000   \n",
       "3                  gini                      10                      10   \n",
       "4                  gini                      10                     100   \n",
       "5                  gini                      10                    1000   \n",
       "6                  gini                      50                      10   \n",
       "7                  gini                      50                     100   \n",
       "8                  gini                      50                    1000   \n",
       "9                  gini                     100                      10   \n",
       "10                 gini                     100                     100   \n",
       "11                 gini                     100                    1000   \n",
       "12              entropy                       5                      10   \n",
       "13              entropy                       5                     100   \n",
       "14              entropy                       5                    1000   \n",
       "15              entropy                      10                      10   \n",
       "16              entropy                      10                     100   \n",
       "17              entropy                      10                    1000   \n",
       "18              entropy                      50                      10   \n",
       "19              entropy                      50                     100   \n",
       "20              entropy                      50                    1000   \n",
       "21              entropy                     100                      10   \n",
       "22              entropy                     100                     100   \n",
       "23              entropy                     100                    1000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'clf__criterion': 'gini', 'clf__max_features'...           0.912454   \n",
       "1   {'clf__criterion': 'gini', 'clf__max_features'...           0.918662   \n",
       "2   {'clf__criterion': 'gini', 'clf__max_features'...           0.919422   \n",
       "3   {'clf__criterion': 'gini', 'clf__max_features'...           0.909033   \n",
       "4   {'clf__criterion': 'gini', 'clf__max_features'...           0.916255   \n",
       "5   {'clf__criterion': 'gini', 'clf__max_features'...           0.919802   \n",
       "6   {'clf__criterion': 'gini', 'clf__max_features'...           0.916002   \n",
       "7   {'clf__criterion': 'gini', 'clf__max_features'...           0.922463   \n",
       "8   {'clf__criterion': 'gini', 'clf__max_features'...           0.922970   \n",
       "9   {'clf__criterion': 'gini', 'clf__max_features'...           0.914608   \n",
       "10  {'clf__criterion': 'gini', 'clf__max_features'...           0.927024   \n",
       "11  {'clf__criterion': 'gini', 'clf__max_features'...           0.927277   \n",
       "12  {'clf__criterion': 'entropy', 'clf__max_featur...           0.911187   \n",
       "13  {'clf__criterion': 'entropy', 'clf__max_featur...           0.920182   \n",
       "14  {'clf__criterion': 'entropy', 'clf__max_featur...           0.919676   \n",
       "15  {'clf__criterion': 'entropy', 'clf__max_featur...           0.913088   \n",
       "16  {'clf__criterion': 'entropy', 'clf__max_featur...           0.920182   \n",
       "17  {'clf__criterion': 'entropy', 'clf__max_featur...           0.921196   \n",
       "18  {'clf__criterion': 'entropy', 'clf__max_featur...           0.914354   \n",
       "19  {'clf__criterion': 'entropy', 'clf__max_featur...           0.925250   \n",
       "20  {'clf__criterion': 'entropy', 'clf__max_featur...           0.927151   \n",
       "21  {'clf__criterion': 'entropy', 'clf__max_featur...           0.916002   \n",
       "22  {'clf__criterion': 'entropy', 'clf__max_featur...           0.926390   \n",
       "23  {'clf__criterion': 'entropy', 'clf__max_featur...           0.928544   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.913203           0.911165         0.912274        0.000842   \n",
       "1            0.920046           0.921176         0.919961        0.001028   \n",
       "2            0.920426           0.919909         0.919919        0.000410   \n",
       "3            0.911809           0.910404         0.910416        0.001133   \n",
       "4            0.920806           0.920289         0.919116        0.002035   \n",
       "5            0.919792           0.921810         0.920468        0.000949   \n",
       "6            0.917511           0.918388         0.917300        0.000986   \n",
       "7            0.924354           0.925231         0.924016        0.001155   \n",
       "8            0.925494           0.927005         0.925156        0.001665   \n",
       "9            0.914090           0.917754         0.915484        0.001619   \n",
       "10           0.929676           0.925992         0.927564        0.001552   \n",
       "11           0.927902           0.927893         0.927690        0.000292   \n",
       "12           0.911049           0.910784         0.911007        0.000167   \n",
       "13           0.919539           0.921049         0.920257        0.000619   \n",
       "14           0.920426           0.920289         0.920130        0.000326   \n",
       "15           0.909275           0.915347         0.912570        0.002505   \n",
       "16           0.921946           0.922443         0.921524        0.000970   \n",
       "17           0.921820           0.920542         0.921186        0.000521   \n",
       "18           0.915864           0.914713         0.914977        0.000644   \n",
       "19           0.926635           0.924978         0.925621        0.000725   \n",
       "20           0.927015           0.927386         0.927184        0.000153   \n",
       "21           0.920172           0.919402         0.918525        0.001812   \n",
       "22           0.929042           0.926879         0.927437        0.001152   \n",
       "23           0.931323           0.927132         0.929000        0.001741   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0                22            0.997085            0.995628   \n",
       "1                14            1.000000            1.000000   \n",
       "2                15            1.000000            1.000000   \n",
       "3                24            0.996515            0.996642   \n",
       "4                16            1.000000            1.000000   \n",
       "5                11            1.000000            1.000000   \n",
       "6                18            0.995248            0.995628   \n",
       "7                 8            1.000000            1.000000   \n",
       "8                 7            1.000000            1.000000   \n",
       "9                19            0.995818            0.996199   \n",
       "10                3            1.000000            1.000000   \n",
       "11                2            1.000000            1.000000   \n",
       "12               23            0.995311            0.995882   \n",
       "13               12            1.000000            1.000000   \n",
       "14               13            1.000000            1.000000   \n",
       "15               21            0.996452            0.996642   \n",
       "16                9            1.000000            1.000000   \n",
       "17               10            1.000000            1.000000   \n",
       "18               20            0.996262            0.995819   \n",
       "19                6            1.000000            1.000000   \n",
       "20                5            1.000000            1.000000   \n",
       "21               17            0.997466            0.996009   \n",
       "22                4            1.000000            1.000000   \n",
       "23                1            1.000000            1.000000   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "0             0.996769          0.996494         0.000626  \n",
       "1             1.000000          1.000000         0.000000  \n",
       "2             1.000000          1.000000         0.000000  \n",
       "3             0.996706          0.996621         0.000079  \n",
       "4             1.000000          1.000000         0.000000  \n",
       "5             1.000000          1.000000         0.000000  \n",
       "6             0.996326          0.995734         0.000446  \n",
       "7             1.000000          1.000000         0.000000  \n",
       "8             1.000000          1.000000         0.000000  \n",
       "9             0.996896          0.996304         0.000446  \n",
       "10            1.000000          1.000000         0.000000  \n",
       "11            1.000000          1.000000         0.000000  \n",
       "12            0.995882          0.995692         0.000269  \n",
       "13            1.000000          1.000000         0.000000  \n",
       "14            1.000000          1.000000         0.000000  \n",
       "15            0.996072          0.996389         0.000237  \n",
       "16            0.999937          0.999979         0.000030  \n",
       "17            1.000000          1.000000         0.000000  \n",
       "18            0.995692          0.995924         0.000244  \n",
       "19            1.000000          1.000000         0.000000  \n",
       "20            1.000000          1.000000         0.000000  \n",
       "21            0.995755          0.996410         0.000754  \n",
       "22            1.000000          1.000000         0.000000  \n",
       "23            1.000000          1.000000         0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'mean_fit_time': np.array([ 29.3565313 ,  21.269967  , 133.63124331,   6.9153959 ,\n",
    "        17.71104503, 151.52857844,   8.77863534,  36.48309787,\n",
    "       353.55145939,  10.91108894,  62.25011571, 604.09919691,\n",
    "         6.63593658,  18.70342271, 125.81533424,   5.24119258,\n",
    "        16.36292839, 141.45259889,   7.89260626,  28.13510966,\n",
    "       273.91240621,   9.28567576,  44.10937214, 356.8786389 ]), 'std_fit_time': np.array([ 0.77492853,  2.90770999, 11.63067571,  0.2850153 ,  0.98171153,\n",
    "       11.44797585,  0.29892795,  3.36007335, 24.64549427,  0.59362711,\n",
    "        6.20874779, 40.38846034,  0.20280898,  0.27479866, 10.47984029,\n",
    "        0.84278515,  1.09157463, 12.09636917,  0.18592348,  1.8504363 ,\n",
    "       17.28253135,  0.37329269,  4.38992519, 99.02754836]), 'mean_score_time': np.array([ 7.33835514,  8.92508117, 24.09163777,  8.09356125,  7.32807509,\n",
    "       19.51869694,  7.59556937,  6.94122378, 14.72333399,  7.78833922,\n",
    "        7.38280678, 15.11881781,  8.04199298,  9.8266205 , 25.7414674 ,\n",
    "        6.29152044,  8.31556026, 18.50302108,  7.92796397,  7.69944366,\n",
    "       13.55180319,  7.96719853,  7.42520293, 11.7625773 ]), 'std_score_time': np.array([0.1306685 , 2.03098883, 2.33742256, 0.45626301, 1.50775737,\n",
    "       3.48599838, 0.21847926, 1.10654311, 2.18771397, 0.04787844,\n",
    "       1.2097773 , 2.10400838, 0.09428107, 0.18964651, 3.18375043,\n",
    "       1.19595878, 0.94813268, 1.95163257, 0.19510065, 1.0073612 ,\n",
    "       1.71333338, 0.16709411, 1.21011483, 2.31377346]), 'param_clf__criterion': np.ma.masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
    "                   'gini', 'gini', 'gini', 'gini', 'gini', 'entropy',\n",
    "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
    "                   'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
    "                   'entropy'],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'param_clf__max_features': np.ma.masked_array(data=[5, 5, 5, 10, 10, 10, 50, 50, 50, 100, 100, 100, 5, 5,\n",
    "                   5, 10, 10, 10, 50, 50, 50, 100, 100, 100],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'param_clf__n_estimators': np.ma.masked_array(data=[10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10, 100,\n",
    "                   1000, 10, 100, 1000, 10, 100, 1000, 10, 100, 1000, 10,\n",
    "                   100, 1000],\n",
    "             mask=[False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False,\n",
    "                   False, False, False, False, False, False, False, False],\n",
    "       fill_value='?',\n",
    "            dtype=object), 'params': [{'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 5, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 10, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 50, 'clf__n_estimators': 1000}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 10}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 100}, {'clf__criterion': 'gini', 'clf__max_features': 100, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 5, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 10, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 50, 'clf__n_estimators': 1000}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 10}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 100}, {'clf__criterion': 'entropy', 'clf__max_features': 100, 'clf__n_estimators': 1000}], 'split0_test_score': np.array([0.91245407, 0.91866211, 0.91942227, 0.90903332, 0.91625491,\n",
    "       0.91980236, 0.91600152, 0.92246294, 0.92296972, 0.91460788,\n",
    "       0.92702395, 0.92727733, 0.91118713, 0.92018244, 0.91967566,\n",
    "       0.91308755, 0.92018244, 0.921196  , 0.91435449, 0.92525022,\n",
    "       0.92715064, 0.91600152, 0.92639047, 0.92854428]), 'split1_test_score': np.array([0.91320324, 0.92004562, 0.92042575, 0.91180943, 0.92080588,\n",
    "       0.91979219, 0.9175114 , 0.92435378, 0.92549417, 0.91409022,\n",
    "       0.92967562, 0.92790167, 0.91104916, 0.91953877, 0.92042575,\n",
    "       0.90927522, 0.92194627, 0.92181956, 0.91586417, 0.92663457,\n",
    "       0.9270147 , 0.92017233, 0.92904207, 0.93132286]), 'split2_test_score': np.array([0.91116462, 0.92117602, 0.91990876, 0.91040426, 0.92028894,\n",
    "       0.92180966, 0.91838804, 0.92523128, 0.92700545, 0.9177544 ,\n",
    "       0.92599164, 0.92789254, 0.91078444, 0.9210493 , 0.92028894,\n",
    "       0.9153466 , 0.92244329, 0.92054239, 0.91471296, 0.92497782,\n",
    "       0.92738563, 0.91940185, 0.92687872, 0.92713218]), 'mean_test_score': np.array([0.91227403, 0.91996114, 0.91991891, 0.91041561, 0.9191164 ,\n",
    "       0.92046798, 0.91730022, 0.92401588, 0.92515628, 0.91548403,\n",
    "       0.92756378, 0.92769049, 0.91100693, 0.9202568 , 0.92013009,\n",
    "       0.91256969, 0.92152391, 0.92118601, 0.91497719, 0.92562088,\n",
    "       0.92718365, 0.91852509, 0.92743707, 0.92899983]), 'std_test_score': np.array([0.00084192, 0.00102804, 0.00040974, 0.00113341, 0.00203455,\n",
    "       0.00094862, 0.00098567, 0.00115515, 0.00166481, 0.00161909,\n",
    "       0.00155163, 0.0002922 , 0.00016709, 0.00061889, 0.00032618,\n",
    "       0.00250546, 0.00097011, 0.00052144, 0.00064403, 0.00072536,\n",
    "       0.00015322, 0.00181211, 0.00115228, 0.00174085]), 'rank_test_score': np.array([22, 14, 15, 24, 16, 11, 18,  8,  7, 19,  3,  2, 23, 12, 13, 21,  9,\n",
    "       10, 20,  6,  5, 17,  4,  1], dtype=np.int32), 'split0_train_score': np.array([0.99708547, 1.        , 1.        , 0.99651524, 1.        ,\n",
    "       1.        , 0.99524805, 1.        , 1.        , 0.99581829,\n",
    "       1.        , 1.        , 0.99531141, 1.        , 1.        ,\n",
    "       0.99645188, 1.        , 1.        , 0.9962618 , 1.        ,\n",
    "       1.        , 0.99746563, 1.        , 1.        ]), 'split1_train_score': np.array([0.99562848, 1.        , 1.        , 0.99664217, 1.        ,\n",
    "       1.        , 0.99562848, 1.        , 1.        , 0.99619868,\n",
    "       1.        , 1.        , 0.99588191, 1.        , 1.        ,\n",
    "       0.99664217, 1.        , 1.        , 0.99581855, 1.        ,\n",
    "       1.        , 0.99600862, 1.        , 1.        ]), 'split2_train_score': np.array([0.99676908, 1.        , 1.        , 0.99670573, 1.        ,\n",
    "       1.        , 0.99632563, 1.        , 1.        , 0.99689579,\n",
    "       1.        , 1.        , 0.99588217, 1.        , 1.        ,\n",
    "       0.99607222, 0.99993665, 1.        , 0.99569211, 1.        ,\n",
    "       1.        , 0.99575546, 1.        , 1.        ]), 'mean_train_score': np.array([0.99649435, 1.        , 1.        , 0.99662105, 1.        ,\n",
    "       1.        , 0.99573405, 1.        , 1.        , 0.99630425,\n",
    "       1.        , 1.        , 0.99569183, 1.        , 1.        ,\n",
    "       0.99638876, 0.99997888, 1.        , 0.99592415, 1.        ,\n",
    "       1.        , 0.9964099 , 1.        , 1.        ]), 'std_train_score': np.array([6.25733424e-04, 0.00000000e+00, 0.00000000e+00, 7.91906591e-05,\n",
    "       0.00000000e+00, 0.00000000e+00, 4.46206266e-04, 0.00000000e+00,\n",
    "       0.00000000e+00, 4.46177186e-04, 0.00000000e+00, 0.00000000e+00,\n",
    "       2.68995301e-04, 0.00000000e+00, 0.00000000e+00, 2.36922985e-04,\n",
    "       2.98640811e-05, 0.00000000e+00, 2.44267970e-04, 0.00000000e+00,\n",
    "       0.00000000e+00, 7.53630262e-04, 0.00000000e+00, 0.00000000e+00])})\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
